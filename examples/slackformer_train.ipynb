{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfa16846",
   "metadata": {},
   "source": [
    "# Slackformer: Basic transformer on WBSlack data\n",
    "\n",
    "Following [this tutorial](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/nlp/ipynb/text_classification_with_transformer.ipynb#scrollTo=HaNCFrk9xtv2) from Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "103f5f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rich extension is already loaded. To reload it, use:\n",
      "  %reload_ext rich\n"
     ]
    }
   ],
   "source": [
    "%load_ext rich\n",
    "\n",
    "from rich import progress\n",
    "import typing\n",
    "import random\n",
    "import json\n",
    "import sklearn\n",
    "import pathlib\n",
    "import itertools\n",
    "import dataclasses\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "SLACK_DATA_DIR = pathlib.Path('..', '..', '..', 'cleanstart', 'slackml', 'data', 'wb-slack')\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Message:\n",
    "    ts: str\n",
    "    channel: str\n",
    "    text: str\n",
    "    user_id: str\n",
    "\n",
    "def dirsize(path) -> int:\n",
    "    return sum(f.stat().st_size for f in path.glob('**/*') if f.is_file())\n",
    "        \n",
    "@dataclasses.dataclass(order=True)\n",
    "class Channel:\n",
    "    name: str\n",
    "\n",
    "    @property\n",
    "    def path(self):\n",
    "        return SLACK_DATA_DIR / self.name\n",
    "        \n",
    "    @property\n",
    "    def size(self) -> int:\n",
    "        return dirsize(self.path)\n",
    "        \n",
    "    @property\n",
    "    def messages(self) -> list[Message]:\n",
    "        chan_path = SLACK_DATA_DIR / self.name\n",
    "        files = chan_path.glob('*.json')\n",
    "        channel_messages = []\n",
    "        for f in sorted(files):\n",
    "            messages = json.load(open(f))\n",
    "            for message in messages:\n",
    "                if message['type'] != 'message' or message.get('subtype'):\n",
    "                    # actual messages from users don't have subtype set\n",
    "                    continue\n",
    "                channel_messages.append(Message(\n",
    "                    message['ts'],\n",
    "                    channel, # TODO: what should this be?\n",
    "                    message['text'],\n",
    "                    message['user']\n",
    "                ))\n",
    "        return channel_messages        \n",
    "\n",
    "def all_slack_channels() -> list[str]:\n",
    "    return sorted(Channel(d.name) for d in SLACK_DATA_DIR.glob('*'))\n",
    "\n",
    "def all_messages(channels: list[Channel]) -> list[Message]:\n",
    "    ms = []\n",
    "    for c in progress.track(channels):\n",
    "        ms += c.messages\n",
    "    return ms\n",
    "\n",
    "# frequency count of users so we can threshold on the minimum number\n",
    "# of messages required for the user to be labeled by our model\n",
    "def get_top_users(all_m: list[Message], min_msg_count=50) -> dict:\n",
    "    user_map = {}\n",
    "    for m in all_m:\n",
    "        if m.user_id in user_map:\n",
    "            user_map[m.user_id] += 1\n",
    "        else:\n",
    "            user_map[m.user_id] = 1\n",
    "    print(\"unique users: \", len(user_map))\n",
    "    um_sort = sorted(user_map.items(), key=lambda x: x[1], reverse=True)\n",
    "    # sort by message count in dataset -\n",
    "    top_users = {}\n",
    "    for i, u in enumerate(um_sort):\n",
    "        if u[1] >= min_msg_count:\n",
    "            top_users[u[0]] = i\n",
    "    print(\"user labels in this model: \", len(top_users))\n",
    "    return top_users\n",
    "    \n",
    "class TrainTest(typing.TypedDict):\n",
    "    train: list[Message]\n",
    "    test: list[Message]\n",
    "\n",
    "def train_test_split(messages: list[Message], train_frac: float) -> TrainTest:\n",
    "    n_train = int(len(messages) * train_frac)\n",
    "    messages = list(messages)\n",
    "    random.shuffle(messages)\n",
    "    return {\n",
    "        'train': messages[:n_train],\n",
    "        'test': messages[n_train:]\n",
    "    }\n",
    "\n",
    "def load_data():\n",
    "    channels = all_slack_channels()\n",
    "    # filter out sentry + deploy-builds\n",
    "    channels = [c for c in channels if not 'sentry' in c.name and not \"deploy-build\" in c.name]\n",
    "    all_m = all_messages(channels)\n",
    "    \n",
    "    # filter for top 215 users (at least 50 messages)\n",
    "    top_users = get_top_users(all_m)\n",
    "    all_m = [m for m in all_m if m.user_id in top_users]\n",
    "    \n",
    "    split = train_test_split(all_m, 0.8)\n",
    "    train, test = split['train'], split['test']\n",
    "    print(\"train: \", len(train), \"test: \", len(test))\n",
    "    return train, test, top_users\n",
    "\n",
    "def usernames():\n",
    "    # pull in user dictionary\n",
    "    import json\n",
    "    u = json.load(open(\"users.json\", 'r'))\n",
    "    names = {}\n",
    "    for e in u:\n",
    "        names[e[\"id\"]] = e[\"name\"]\n",
    "    print(\"total users: \", len(names))\n",
    "    return names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fca6d4",
   "metadata": {},
   "source": [
    "## Define a transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9f95303",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3b92d0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">225006</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m225006\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Shawn what is channel in the class def supposed to be to start? \n",
    "# if one doesn't run this block, the class definition fails on \"channel\"\n",
    "channel = Channel('ceo-ama')\n",
    "channel.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cbaec45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b2e55e5da774ea5a0b4186ec9551fd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique users:  1083\n",
      "user labels in this model:  215\n",
      "train:  117805 test:  29452\n"
     ]
    }
   ],
   "source": [
    "train, test, top_users = load_data()\n",
    "train_text = [t.text for t in train]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0f1ef7",
   "metadata": {},
   "source": [
    "## Fit embedding to training text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b909ae2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "def custom_standardization(input_data):\n",
    "  return tf.strings.lower(input_data)\n",
    "  #stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "  #return tf.strings.regex_replace(stripped_html,\n",
    "      #                            '[%s]' % re.escape(string.punctuation), '')\n",
    "\n",
    "\n",
    "# Vocabulary size and number of words in a sequence.\n",
    "vocab_size = 20000\n",
    "# TODO these should be the same\n",
    "sequence_length = 200\n",
    "maxlen=200\n",
    "\n",
    "# Use the text vectorization layer to normalize, split, and map strings to\n",
    "# integers. Note that the layer uses the custom standardization defined above.\n",
    "# Set maximum_sequence length as all samples are not of the same length.\n",
    "vectorize_layer = TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)\n",
    "\n",
    "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
    "#text_ds = train.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(train_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e6986d",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6d64ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 64  # Embedding size for each token\n",
    "num_heads = 6  # Number of attention heads\n",
    "ff_dim = 64  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "inputs = layers.Input(shape=(1,), dtype=tf.string)\n",
    "x = vectorize_layer(inputs)\n",
    "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "x = embedding_layer(x)\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.1)(x) # was 0.1\n",
    "x = layers.Dense(20, activation=\"relu\")(x) # was 20\n",
    "x = layers.Dropout(0.1)(x)\n",
    "outputs = layers.Dense(215, activation=\"softmax\")(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48c96ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train =[top_users[t.user_id] for t in train]\n",
    "x_val = [t.text for t in test]\n",
    "y_val = [top_users[t.user_id] for t in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aff29205",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ff43da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1841/1841 [==============================] - 962s 522ms/step - loss: 4.2473 - accuracy: 0.1245 - val_loss: 3.8026 - val_accuracy: 0.1834\n"
     ]
    }
   ],
   "source": [
    "# up to 5 epochs!!\n",
    "model.compile(\n",
    "    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "history = model.fit(\n",
    "    train_text, y_train, batch_size=64, epochs=1, validation_data=(x_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d1796322",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embedding_2_layer_call_fn, embedding_2_layer_call_and_return_conditional_losses, embedding_3_layer_call_fn, embedding_3_layer_call_and_return_conditional_losses, multi_head_attention_1_layer_call_fn while saving (showing 5 of 26). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_test_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_test_model/assets\n"
     ]
    }
   ],
   "source": [
    "## Don't forget to save\n",
    "### try a unique descriptive name ;)\n",
    "model.save(\"my_test_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d56ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# more fun ideas to try: \n",
    "\n",
    "# tried:\n",
    "# - bigger batch size! meh, speed not much diffe\n",
    "# - more epochs\n",
    "# - more attention heads\n",
    "# - one more dense layer\n",
    "\n",
    "# to try:\n",
    "# - other loss\n",
    "# - stop words\n",
    "# - punctuation?!\n",
    "# - different min cutoff\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
