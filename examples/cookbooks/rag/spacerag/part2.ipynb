{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/wandb/weave/blob/anish/add-spacerag-example/examples/cookbooks/rag/spacerag/part2.ipynb)\n",
    "<!--- @wandbcode{weave-spacerag-cookbook} -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_COLAB = False\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    import os\n",
    "    os.environ[\"WANDB_API_KEY\"] = userdata.get(\"WANDB_API_KEY\")\n",
    "    os.environ[\"TOGETHER_API_KEY\"] = userdata.get(\"TOGETHER_API_KEY\")\n",
    "    os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "repo_url = \"https://github.com/wandb/weave.git\"\n",
    "target_folder = \"weave_cookbooks\"\n",
    "subdirectory = \"examples/cookbooks\"\n",
    "branch = \"anish/add-spacerag-example\"\n",
    "\n",
    "if not os.path.exists(target_folder) and IN_COLAB:\n",
    "    print(f\"Cloning repository: {repo_url}\")\n",
    "\n",
    "    # Clone the entire repository to a temporary folder\n",
    "    temp_folder = \"temp_weave_repo\"\n",
    "    subprocess.run([\"git\", \"clone\", \"--depth\", \"1\", \"--branch\", branch, repo_url, temp_folder], check=True)\n",
    "\n",
    "    # Move the desired subdirectory to the target folder\n",
    "    shutil.move(os.path.join(temp_folder, subdirectory), target_folder)\n",
    "\n",
    "    # Remove the temporary folder\n",
    "    shutil.rmtree(temp_folder)\n",
    "\n",
    "    print(f\"Successfully cloned {subdirectory} from branch '{branch}' to {target_folder}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"Folder '{target_folder}' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(target_folder) and IN_COLAB:\n",
    "    %cd weave_cookbooks/rag/spacerag\n",
    "    !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weave\n",
    "from weave import Evaluation\n",
    "import os\n",
    "import numpy as np\n",
    "import faiss\n",
    "from openai import OpenAI\n",
    "from together import Together\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weave.init('space_rag_example')\n",
    "\n",
    "# SERVE MODEL FROM TOGETHER ENDPOINT\n",
    "client = Together(api_key=os.environ.get(\"TOGETHER_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHUNK DATA FROM EXTERNAL KNOWLEDGEBASE\n",
    "@weave.op\n",
    "def get_chunked_data(file):\n",
    "    # get data - file\n",
    "    with open(file, 'r') as file:\n",
    "        # Read the contents of the file into a variable\n",
    "        text = file.read()\n",
    "\n",
    "    # split doc into chunks\n",
    "    chunk_size = 2048\n",
    "    chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "    return chunks\n",
    "\n",
    "# EMBED DATA\n",
    "@weave.op\n",
    "def get_text_embedding(input):\n",
    "    api_key_openai = os.environ[\"OPENAI_API_KEY\"]\n",
    "    client = OpenAI(api_key=api_key_openai)\n",
    "    \n",
    "    response = client.embeddings.create(\n",
    "        model=\"text-embedding-3-small\",\n",
    "        input=input\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "# MAKE VECTORDB\n",
    "@weave.op\n",
    "def make_vector_db(file):\n",
    "    # get chunked data from function get_chunked_data()\n",
    "    chunks = get_chunked_data(file)\n",
    "    # embed data\n",
    "    text_embeddings = np.array([get_text_embedding(chunk) for chunk in chunks])\n",
    "    # embed data into vectordb\n",
    "    d = text_embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(d)\n",
    "    index.add(text_embeddings)\n",
    "    return index, chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = './data/space.txt'\n",
    "index, chunks = make_vector_db(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER QUESTION\n",
    "@weave.op\n",
    "def predict(model, prompt):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\":\"user\",\"content\":prompt}],\n",
    "        temperature=0.5,\n",
    "        top_p=1,\n",
    "        max_tokens=1024,\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    answer = []\n",
    "    for chunk in completion:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            answer.append(chunk.choices[0].delta.content)\n",
    "    \n",
    "    result = ''.join(answer)\n",
    "    print(result)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RETRIEVE CHUNKS SIMILAR TO THE QUESTION\n",
    "@weave.op\n",
    "def retrieve_context(question: str) -> list:\n",
    "    question_embeddings = np.array([get_text_embedding(question)])\n",
    "    # Retrieve similar chunks from the vectorDB\n",
    "    D, I = index.search(question_embeddings, k=2) \n",
    "    retrieved_chunk = [chunks[i] for i in I.tolist()[0]]\n",
    "    return retrieved_chunk\n",
    "    \n",
    "class SpaceRAGModel(weave.Model):\n",
    "    model: str\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, question: str):\n",
    "        retrieved_chunk = retrieve_context(question)\n",
    "        print(\"Question: \"+question)\n",
    "\n",
    "        # Combine context and question in a prompt\n",
    "        prompt = f\"\"\"\n",
    "        Use this context to answer the question, don't use any prior knowledge.\n",
    "        Be concise in your answers.\n",
    "        ---------------------\n",
    "        {retrieved_chunk}\n",
    "        ---------------------\n",
    "        Question: {question}\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "        answer = predict(self.model, prompt)\n",
    "        print(\"___________________________\")\n",
    "        return {'answer': answer, 'retrieved_chunk': retrieved_chunk}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_dict(input_string):\n",
    "    # Use regular expressions to find all JSON-like objects in the string\n",
    "    json_objects = re.findall(r'\\{.*?\\}', input_string)\n",
    "\n",
    "    # Initialize an empty dictionary to store the combined results\n",
    "    combined_dict = {}\n",
    "\n",
    "    for obj in json_objects:\n",
    "        try:\n",
    "            # Parse each JSON object\n",
    "            parsed_dict = json.loads(obj)\n",
    "            # Update the combined dictionary with the parsed data\n",
    "            combined_dict.update(parsed_dict)\n",
    "        except (ValueError, json.JSONDecodeError) as e:\n",
    "            print(f\"Error processing part: {obj}\\nError: {e}\")\n",
    "\n",
    "    return combined_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ref = weave.ref(\"weave:///lavanyashukla/spacedata/object/space_dataset_llm_comprehensive:VBd5Ys7b3hGFmJJqdGTATVQYgKKrg70EiNV5FdpwFxs\").get()\n",
    "small_questions = dataset_ref.rows[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_nan_in_dict(result):\n",
    "    for key in result:\n",
    "        if isinstance(result[key], float) and np.isnan(result[key]):\n",
    "            result[key] = 0\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate with an LLM\n",
    "@weave.op\n",
    "def llm_judge_scorer(ground_truth: str, model_output: dict) -> dict:\n",
    "    scorer_llm = \"meta-llama/Meta-Llama-3-70B-Instruct-Turbo\"\n",
    "    answer = model_output['answer']\n",
    "    retrieved_chunk = model_output['retrieved_chunk']\n",
    "\n",
    "    eval_rubrics = [\n",
    "    {\n",
    "        \"metric\": \"concise\",\n",
    "        \"rubrics\": \"\"\"\n",
    "        Score 1: The answer is rambling and difficult to understand.\n",
    "        Score 2: The answer is somewhat readable, engaging, or long winded.\n",
    "        Score 3: The answer is mostly easy to understand, and is somewhat consice.\n",
    "        Score 4: The answer is completely concise, readable and engaging.\n",
    "        \"\"\",\n",
    "    },\n",
    "    # {\n",
    "    #     \"metric\": \"relevant\",\n",
    "    #     \"rubrics\": \"\"\"\n",
    "    #     Score 1: The answer is not relevant to the original text. \n",
    "    #     Score 2: The answer is somewhat relevant to the original text, but has significant flaws.\n",
    "    #     Score 3: The answer is mostly relevant to the original text, and effectively conveys its main ideas and arguments.\n",
    "    #     Score 4: The answer is completely relevant to the original text, and provides additional value or insight.\n",
    "    #     \"\"\",\n",
    "    # },\n",
    "    # {\n",
    "    #     \"metric\": \"accurate\",\n",
    "    #     \"rubrics\": \"\"\"\n",
    "    #     Compare the factual content of the model's answer with the correct answer. Ignore any differences in style, grammar, or punctuation.\n",
    "    #     Score 1: There is a disagreement between the model's answer and the correct answer.\n",
    "    #     Score 2: The model's answer is a subset of the correct answer and is fully consistent with it.\n",
    "    #     Score 3: The answers differ, but these differences don't matter from the perspective of factuality.\n",
    "    #     Score 4: The model's answer contains all the same details as the correct answer.\n",
    "    #     \"\"\",\n",
    "    # },\n",
    "]\n",
    "\n",
    "    scoring_prompt = \"\"\"\n",
    "    You have the correct answer, original text and the model's answer below.\n",
    "    Based on the specified evaluation metric and rubric, assign an integer score between 1 and 4 to the summary. \n",
    "    Then, return a JSON object with the metric name as the key and the evaluation score as the value. Don't output anything else.\n",
    "\n",
    "    # Evaluation metric:\n",
    "    {metric}\n",
    "\n",
    "    # Evaluation rubrics:\n",
    "    {rubrics}\n",
    "\n",
    "    # Correct Answer\n",
    "    {ground_truth}\n",
    "    \n",
    "    # Original Text\n",
    "    {retrieved_chunk}\n",
    "\n",
    "    # Model Answer\n",
    "    {model_answer}\n",
    "\n",
    "    \"\"\"\n",
    "    evals = \"\"\n",
    "    for i in eval_rubrics:\n",
    "        eval_output = predict(scorer_llm,\n",
    "            scoring_prompt.format(\n",
    "                ground_truth=ground_truth, retrieved_chunk=retrieved_chunk, model_answer=answer,\n",
    "                metric=i[\"metric\"], rubrics=i[\"rubrics\"]\n",
    "            ))+\" \"\n",
    "        evals+=eval_output\n",
    "    # evals_json = format_string_to_json(evals)\n",
    "    evals_dict = string_to_dict(evals)\n",
    "    # print(\"___________________________\")\n",
    "    # print(evals_dict)\n",
    "    # print(\"___________________________\")\n",
    "    return evals_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ragas_score(question, ground_truth, model_output):\n",
    "#     from datasets import Dataset\n",
    "#     from ragas import evaluate\n",
    "#     from ragas.metrics import faithfulness, answer_relevancy, answer_correctness, context_recall, context_precision\n",
    "\n",
    "#     metric_modules = [\n",
    "#         answer_relevancy,\n",
    "#         context_recall,\n",
    "#     ]\n",
    "    \n",
    "#     # Convert the retrieved_chunk to a list of strings\n",
    "#     contexts = [str(chunk) for chunk in model_output[\"retrieved_chunk\"]]\n",
    "    \n",
    "#     qa_dataset = Dataset.from_dict(\n",
    "#         {\n",
    "#             \"question\": [question],\n",
    "#             \"ground_truth\": [ground_truth],\n",
    "#             \"answer\": [model_output[\"answer\"]],\n",
    "#             \"contexts\": [contexts],  # Wrap contexts in another list\n",
    "#         }\n",
    "#     )\n",
    "#     result = evaluate(qa_dataset, metrics=metric_modules,\n",
    "#                       raise_exceptions=False)\n",
    "#     return replace_nan_in_dict(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def tonic_validate_score(question: str, ground_truth: str, model_output: dict) -> dict:\n",
    "    from tonic_validate import Benchmark, ValidateScorer\n",
    "    from tonic_validate.metrics import DuplicationMetric, AnswerSimilarityMetric, AnswerConsistencyMetric\n",
    "\n",
    "    metric_modules = [DuplicationMetric(), AnswerSimilarityMetric(), AnswerConsistencyMetric()]\n",
    "\n",
    "    def get_llm_response(question):\n",
    "        return {\n",
    "            \"llm_answer\": model_output['answer'],\n",
    "            \"llm_context_list\": (\n",
    "                [model_output['retrieved_chunk']]\n",
    "                if isinstance(model_output['retrieved_chunk'], str)\n",
    "                else model_output['retrieved_chunk']\n",
    "            ),\n",
    "        }\n",
    "\n",
    "    benchmark = Benchmark(questions=[question], answers=[ground_truth])\n",
    "    scorer = ValidateScorer(metrics=metric_modules)\n",
    "    run = scorer.score(benchmark, get_llm_response)\n",
    "    return run.run_data[0].scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"meta-llama/Meta-Llama-3-70B-Instruct-Turbo\",\n",
    "        #   \"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
    "        #   \"Snowflake/snowflake-arctic-instruct\",\n",
    "        #   \"mistralai/Mixtral-8x22B-Instruct-v0.1\"]\n",
    "]\n",
    "for model in models:\n",
    "    rag_model = SpaceRAGModel(model=model)\n",
    "    evaluation = Evaluation(dataset=small_questions, scorers=[\n",
    "    llm_judge_scorer,\n",
    "    # ragas_score,\n",
    "    tonic_validate_score\n",
    "])\n",
    "    print(f\"RAG Model: {model}\")\n",
    "    await evaluation.evaluate(rag_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
