{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Chatbot to assist an English Teacher\n",
    "\n",
    "In this tutorial, we'll build a chatbot to assist a teacher with grading student assignments and help clarify student questions regarding the syllabus and study materials using **retrieval augmented generation (RAG)**.\n",
    "\n",
    "We'll build our RAG chatbot with these tools:\n",
    "\n",
    "- LlamaIndex as the data framework for building the RAG application\n",
    "- Groq as an LLM vendor\n",
    "- Instructor to get structured output with a consistent schema from our LLM \n",
    "- Weave for tracking and evaluating LLM applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU rich\n",
    "!pip install -qU wandb weave\n",
    "!pip install -qU llama-index\n",
    "!pip install -qU llama-index-embeddings-huggingface llama-index-llms-groq\n",
    "!pip install -qU wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from getpass import getpass\n",
    "\n",
    "import rich\n",
    "import weave\n",
    "import wget\n",
    "from llama_index.llms.groq import Groq\n",
    "from llama_index.core import Settings, SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import VectorStoreIndex\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_api_key = getpass(\"Enter your Groq API key: \")\n",
    "os.environ[\"GROQ_API_KEY\"] = groq_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in as Weights & Biases user: geekyrakshit.\n",
      "View Weave data at https://wandb.ai/geekyrakshit/llamaindex-groq-rag/weave\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<weave.weave_client.WeaveClient at 0x722f35df97b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_name = \"llamaindex-groq-rag\" # @param {type:\"string\"}\n",
    "\n",
    "weave.init(project_name=project_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaData(weave.Model):\n",
    "    source_document_url: str = \"https://huggingface.co/datasets/wandb/weave_cookbook_datasets/resolve/main/flamingos_ncert.zip\"\n",
    "    embedding_model: str = \"BAAI/bge-small-en-v1.5\"\n",
    "    splitter_buffer_size: int = 1\n",
    "    splitter_breakpoint_percentile_threshold: int = 95\n",
    "    vector_index_persist_dir: str = \"./vector_embedding_storage\"\n",
    "    similarity_top_k: int = 10\n",
    "\n",
    "\n",
    "metadata = MetaData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_file = wget.download(metadata.source_document_url)\n",
    "with zipfile.ZipFile(zip_file, \"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"./\")\n",
    "os.remove(zip_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.10/site-packages/llama_index/core/readers/file/base.py:663: UserWarning: Specified num_workers exceed number of CPUs in the system. Setting `num_workers` down to the maximum CPU count.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "reader = SimpleDirectoryReader(input_dir=\"chapters\")\n",
    "documents = reader.load_data(num_workers=4, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = HuggingFaceEmbedding(model_name=metadata.embedding_model)\n",
    "llm = Groq(model=\"llama3-8b-8192\", api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 23/23 [00:01<00:00, 19.55it/s]\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:00<00:00, 25.59it/s]\n",
      "Generating embeddings: 100%|██████████| 33/33 [00:01<00:00, 28.89it/s]\n",
      "Generating embeddings: 100%|██████████| 43/43 [00:01<00:00, 32.26it/s]\n",
      "Generating embeddings: 100%|██████████| 37/37 [00:01<00:00, 21.18it/s]\n",
      "Generating embeddings: 100%|██████████| 35/35 [00:01<00:00, 27.07it/s]\n",
      "Generating embeddings: 100%|██████████| 23/23 [00:01<00:00, 18.69it/s]\n",
      "Generating embeddings: 100%|██████████| 14/14 [00:01<00:00, 12.63it/s]\n",
      "Generating embeddings: 100%|██████████| 23/23 [00:01<00:00, 15.96it/s]\n",
      "Generating embeddings: 100%|██████████| 20/20 [00:01<00:00, 16.35it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:01<00:00, 18.64it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:01<00:00, 15.65it/s]\n",
      "Generating embeddings: 100%|██████████| 26/26 [00:01<00:00, 23.80it/s]\n",
      "Generating embeddings: 100%|██████████| 26/26 [00:01<00:00, 18.37it/s]\n",
      "Generating embeddings: 100%|██████████| 25/25 [00:01<00:00, 19.86it/s]\n",
      "Generating embeddings: 100%|██████████| 28/28 [00:01<00:00, 22.77it/s]\n",
      "Generating embeddings: 100%|██████████| 18/18 [00:00<00:00, 18.62it/s]\n",
      "Generating embeddings: 100%|██████████| 28/28 [00:01<00:00, 23.03it/s]\n",
      "Generating embeddings: 100%|██████████| 13/13 [00:00<00:00, 22.17it/s]\n",
      "Generating embeddings: 100%|██████████| 27/27 [00:01<00:00, 23.46it/s]\n",
      "Generating embeddings: 100%|██████████| 31/31 [00:01<00:00, 24.84it/s]\n",
      "Generating embeddings: 100%|██████████| 19/19 [00:01<00:00, 15.53it/s]\n",
      "Generating embeddings: 100%|██████████| 14/14 [00:00<00:00, 17.78it/s]\n",
      "Generating embeddings: 100%|██████████| 28/28 [00:01<00:00, 24.47it/s]\n",
      "Generating embeddings: 100%|██████████| 24/24 [00:01<00:00, 19.65it/s]\n",
      "Generating embeddings: 100%|██████████| 18/18 [00:00<00:00, 19.32it/s]\n",
      "Generating embeddings: 100%|██████████| 23/23 [00:00<00:00, 26.21it/s]\n",
      "Generating embeddings: 100%|██████████| 29/29 [00:01<00:00, 18.51it/s]\n",
      "Generating embeddings: 100%|██████████| 28/28 [00:01<00:00, 24.96it/s]\n",
      "Generating embeddings: 100%|██████████| 22/22 [00:01<00:00, 14.74it/s]\n",
      "Generating embeddings: 100%|██████████| 22/22 [00:01<00:00, 17.34it/s]\n",
      "Generating embeddings: 100%|██████████| 23/23 [00:00<00:00, 23.87it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:01<00:00, 14.31it/s]\n",
      "Generating embeddings: 100%|██████████| 18/18 [00:00<00:00, 19.42it/s]\n",
      "Generating embeddings: 100%|██████████| 26/26 [00:01<00:00, 20.64it/s]\n",
      "Generating embeddings: 100%|██████████| 20/20 [00:01<00:00, 14.86it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:01<00:00, 15.28it/s]\n",
      "Generating embeddings: 100%|██████████| 19/19 [00:01<00:00, 15.93it/s]\n",
      "Generating embeddings: 100%|██████████| 19/19 [00:01<00:00, 11.20it/s]\n",
      "Generating embeddings: 100%|██████████| 22/22 [00:01<00:00, 15.51it/s]\n",
      "Generating embeddings: 100%|██████████| 26/26 [00:01<00:00, 17.28it/s]\n",
      "Generating embeddings: 100%|██████████| 24/24 [00:00<00:00, 30.72it/s]\n",
      "Generating embeddings: 100%|██████████| 14/14 [00:01<00:00, 12.57it/s]\n",
      "Generating embeddings: 100%|██████████| 25/25 [00:01<00:00, 15.17it/s]\n",
      "Generating embeddings: 100%|██████████| 16/16 [00:01<00:00, 13.25it/s]\n",
      "Generating embeddings: 100%|██████████| 23/23 [00:01<00:00, 22.47it/s]\n",
      "Generating embeddings: 100%|██████████| 27/27 [00:00<00:00, 29.57it/s]\n",
      "Generating embeddings: 100%|██████████| 22/22 [00:01<00:00, 20.77it/s]\n",
      "Generating embeddings: 100%|██████████| 13/13 [00:00<00:00, 14.83it/s]\n",
      "Generating embeddings: 100%|██████████| 11/11 [00:01<00:00, 10.04it/s]\n",
      "Generating embeddings: 100%|██████████| 19/19 [00:01<00:00, 12.09it/s]\n",
      "Generating embeddings: 100%|██████████| 20/20 [00:01<00:00, 15.72it/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00, 32.65it/s]\n",
      "Generating embeddings: 100%|██████████| 18/18 [00:01<00:00, 16.98it/s]\n",
      "Generating embeddings: 100%|██████████| 19/19 [00:01<00:00, 16.77it/s]\n",
      "Generating embeddings: 100%|██████████| 13/13 [00:01<00:00, 11.80it/s]\n",
      "Generating embeddings: 100%|██████████| 17/17 [00:01<00:00, 14.09it/s]\n",
      "Generating embeddings: 100%|██████████| 28/28 [00:01<00:00, 21.96it/s]\n",
      "Generating embeddings: 100%|██████████| 20/20 [00:01<00:00, 14.92it/s]\n",
      "Generating embeddings: 100%|██████████| 20/20 [00:01<00:00, 18.09it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:01<00:00, 16.45it/s]\n",
      "Generating embeddings: 100%|██████████| 16/16 [00:01<00:00, 13.76it/s]\n",
      "Generating embeddings: 100%|██████████| 23/23 [00:01<00:00, 18.75it/s]\n",
      "Generating embeddings: 100%|██████████| 22/22 [00:01<00:00, 17.91it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:01<00:00, 15.32it/s]\n",
      "Generating embeddings: 100%|██████████| 18/18 [00:01<00:00, 12.76it/s]\n",
      "Parsing nodes: 100%|██████████| 66/66 [01:18<00:00,  1.18s/it]\n"
     ]
    }
   ],
   "source": [
    "splitter = SemanticSplitterNodeParser(\n",
    "    buffer_size=metadata.splitter_buffer_size,\n",
    "    breakpoint_percentile_threshold=metadata.splitter_breakpoint_percentile_threshold,\n",
    "    embed_model=embed_model,\n",
    ")\n",
    "nodes = splitter.get_nodes_from_documents(documents, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 66/66 [00:00<00:00, 1678.37it/s]\n",
      "Generating embeddings: 100%|██████████| 66/66 [00:26<00:00,  2.53it/s]\n"
     ]
    }
   ],
   "source": [
    "vector_index = VectorStoreIndex.from_documents(\n",
    "    documents, show_progress=True, node_parser=nodes\n",
    ")\n",
    "vector_index.storage_context.persist(persist_dir=metadata.vector_index_persist_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = vector_index.as_query_engine(\n",
    "    llm=llm,\n",
    "    similarity_top_k=metadata.similarity_top_k,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">The mood in the classroom was solemn and somber.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "The mood in the classroom was solemn and somber.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = (\n",
    "    \"\"\"In the story 'The Last Lesson', what was the mood in the classroom when M. Hamel gave his last French lesson?\"\"\"\n",
    ")\n",
    "response = query_engine.query(query).response\n",
    "\n",
    "rich.print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
