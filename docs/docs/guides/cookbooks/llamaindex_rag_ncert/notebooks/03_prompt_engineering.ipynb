{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Task-specific Assistants using Prompt Engineering\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/wandb/weave/blob/master/docs/docs/guides/cookbooks/llamaindex_rag_ncert/notebooks/03_prompt_engineering.ipynb)\n",
    "\n",
    "Now that we have a functional RAG pipeline, let's use some basic prompt engineering to make it a little more helpful. We need our teaching assistant to be able to perform the following tasks:\n",
    "\n",
    "- emulating the ideal response of a student to a question\n",
    "- emulating the teacher's response to a question from a student.\n",
    "- help the teacher grade the answer given by a student to a question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the dependencies\n",
    "\n",
    "First, let us install all the libraries that we would need to build the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU rich\n",
    "!pip install -U instructor\n",
    "!pip install -qU wandb\n",
    "!pip install -qU git+https://github.com/wandb/weave.git@feat/groq\n",
    "!pip install -qU llama-index groq\n",
    "!pip install -qU llama-index-embeddings-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import instructor\n",
    "import rich\n",
    "import wandb\n",
    "from groq import Groq\n",
    "from llama_index.core import ServiceContext, StorageContext, load_index_from_storage\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from pydantic import BaseModel\n",
    "\n",
    "import weave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Retreiver from the Vector Store Index\n",
    "\n",
    "Retrievers are responsible for fetching the most relevant context given a user query or chat message. We are going to use the [`as_retriever`](https://docs.llamaindex.ai/en/stable/api_reference/indices/document_summary/?h=as_retriever#llama_index.core.indices.DocumentSummaryIndex.as_retriever) instead of the `as_query_engine` in the previous recipe to build our retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weave.init(project_name=\"groq-rag\")\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=None)\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"./vector_embedding_storage\")\n",
    "index = load_index_from_storage(storage_context, service_context=service_context)\n",
    "\n",
    "retreival_engine = index.as_retriever(\n",
    "    service_context=service_context,\n",
    "    similarity_top_k=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use this retriever to retrieve a list of [`NodeWithScore`](https://docs.llamaindex.ai/en/stable/api_reference/schema/?h=nodewithscore#llama_index.core.schema.NodeWithScore) objects which represent units of retrieved text segments. The nodes are arranged in descending order of similarity score, hence we can simply pick the first node in the list as our context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (\n",
    "    \"\"\"what was the mood in the classroom when M. Hamel gave his last French lesson?\"\"\"\n",
    ")\n",
    "response = retreival_engine.retrieve(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapter_name = (\n",
    "    response[0].node.metadata[\"file_name\"].split(\".\")[0].replace(\"_\", \" \").title()\n",
    ")\n",
    "context = response[0].node.text\n",
    "\n",
    "rich.print(f\"{chapter_name=}\")\n",
    "rich.print(f\"{context=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROQ_API_KEY = getpass(\"Enter your GROQ API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Simple Doubt-clearing Assistant\n",
    "\n",
    "We're going to use [`weave.Model`](https://wandb.github.io/weave/guides/core-types/models) to write our assitants. A `weave.Model` is a combination of data (which can include configuration, trained model weights, or other information) and code that defines how the model operates. By structuring your code to be compatible with this API, you benefit from a structured way to version your application so you can more systematically keep track of your experiments.\n",
    "\n",
    "Let's use a simple prompt template to build a doubt-clearning assistant..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnglishDoubtClearningAssistant(weave.Model):\n",
    "    model: str = \"llama3-8b-8192\"\n",
    "    _groq_client: Optional[Groq] = None\n",
    "\n",
    "    def __init__(self, model: Optional[str] = None):\n",
    "        super().__init__()\n",
    "        self.model = model if model is not None else self.model\n",
    "        self._groq_client = Groq(api_key=GROQ_API_KEY)\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, question: str, context: str) -> Tuple[str, str]:\n",
    "        chat_completion = self._groq_client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"\"\"\n",
    "You are a student in a class and your teacher has asked you to answer the following question.\n",
    "You have to write the answer in the given word limit.\"\"\",\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"\"\"\n",
    "We have provided context information below.\n",
    "\n",
    "---\n",
    "{context}\n",
    "---\n",
    "\n",
    "Answer the following question within 50-150 words:\n",
    "\n",
    "```\n",
    "{query}\n",
    "```\"\"\",\n",
    "                },\n",
    "            ],\n",
    "            model=self.model,\n",
    "        )\n",
    "        return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doubt_clearning_assistant = EnglishDoubtClearningAssistant()\n",
    "\n",
    "rich.print(doubt_clearning_assistant.predict(question=query, context=context))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following image shows a Weave trace for `EnglishDoubtClearningAssistant.predict` showing the versioned `EnglishDoubtClearningAssistant` model object.\n",
    "\n",
    "![](../images/weave_dashboard_doubt_clearing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Simple Assistant for Generating Student Response\n",
    "\n",
    "Let's use another simple prompt template to build a student response generating assistant that generates an ideal answer to a question dependening on the total marks that can be awarded for the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnglishStudentResponseAssistant(weave.Model):\n",
    "    model: str = \"llama3-8b-8192\"\n",
    "    _groq_client: Optional[Groq] = None\n",
    "\n",
    "    def __init__(self, model: Optional[str] = None):\n",
    "        super().__init__()\n",
    "        self.model = model if model is not None else self.model\n",
    "        self._groq_client = Groq(api_key=GROQ_API_KEY)\n",
    "\n",
    "    @weave.op()\n",
    "    def get_prompt(\n",
    "        self, question: str, context: str, word_limit_min: int, word_limit_max: int\n",
    "    ) -> Tuple[str, str]:\n",
    "        system_prompt = \"\"\"\n",
    "You are a student in a class and your teacher has asked you to answer the following question.\n",
    "You have to write the answer in the given word limit.\"\"\"\n",
    "        user_prompt = f\"\"\"\n",
    "We have provided context information below.\n",
    "\n",
    "---\n",
    "{context}\n",
    "---\n",
    "\n",
    "Answer the following question within {word_limit_min}-{word_limit_max} words:\n",
    "\n",
    "---\n",
    "{question}\n",
    "---\"\"\"\n",
    "        return system_prompt, user_prompt\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, question: str, total_marks: int) -> str:\n",
    "        response = retreival_engine.retrieve(question)\n",
    "        context = response[0].node.text\n",
    "        if total_marks < 3:\n",
    "            word_limit_min = 5\n",
    "            word_limit_max = 50\n",
    "        elif total_marks < 5:\n",
    "            word_limit_min = 50\n",
    "            word_limit_max = 100\n",
    "        else:\n",
    "            word_limit_min = 100\n",
    "            word_limit_max = 200\n",
    "        system_prompt, user_prompt = self.get_prompt(\n",
    "            question, context, word_limit_min, word_limit_max\n",
    "        )\n",
    "        chat_completion = self._groq_client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": system_prompt,\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": user_prompt,\n",
    "                },\n",
    "            ],\n",
    "            model=self.model,\n",
    "        )\n",
    "        return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_response_assistant = EnglishDoubtClearningAssistant()\n",
    "ideal_student_response = student_response_assistant.predict(question=query, context=context, total_marks=5)\n",
    "rich.print(ideal_student_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following image shows a Weave trace for `EnglishStudentResponseAssistant.predict` showing the versioned `EnglishStudentResponseAssistant` model object.\n",
    "\n",
    "![](../images/weave_dashboard_student_response.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Simple Answer-grading Assistant\n",
    "\n",
    "In order to get a holistic evaluation from our assistant, we would need to get the LLM response structured into a consistent schema like a `pydantic.BaseModel`. In order to acheive this we're going to use the [Instructor](https://python.useinstructor.com/) library with our LLM.\n",
    "\n",
    "Let's first install Instructor..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to use another simple prompt template to build a answer grading assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradeExtractor(BaseModel):\n",
    "    question: str\n",
    "    student_answer: str\n",
    "    marks: float\n",
    "    total_marks: float\n",
    "    feedback: str\n",
    "\n",
    "\n",
    "class EnglishGradingAssistant(EnglishStudentResponseAssistant):\n",
    "    model: str = \"llama3-8b-8192\"\n",
    "    _groq_client: Optional[Groq] = None\n",
    "    _instructor_groq_client: Optional[instructor.Instructor] = None\n",
    "\n",
    "    def __init__(self, model: Optional[str] = None):\n",
    "        super().__init__(model=model)\n",
    "        self.model = model if model is not None else self.model\n",
    "        self._instructor_groq_client = instructor.from_groq(\n",
    "            Groq(api_key=GROQ_API_KEY)\n",
    "        )\n",
    "\n",
    "    @weave.op()\n",
    "    def get_prompt_for_grading(\n",
    "        self,\n",
    "        question: str,\n",
    "        context: str,\n",
    "        total_marks: int,\n",
    "        student_answer: Optional[str] = None,\n",
    "    ) -> Tuple[str, str]:\n",
    "        system_prompt = \"\"\"\n",
    "You are a helpful assistant to an English teacher meant to grade the answer given by a student to a question.\n",
    "You have to extract the question , the student's answer, the marks awarded to the student out of total marks,\n",
    "the total marks and a contructive feedback to the student's answer with regards to how accurate it is with\n",
    "respect to the context.\n",
    "        \"\"\"\n",
    "        student_answer = (\n",
    "            self.predict(question, total_marks)\n",
    "            if student_answer is None\n",
    "            else student_answer\n",
    "        )\n",
    "        user_prompt = f\"\"\"\n",
    "We have provided context information below.\n",
    "\n",
    "---\n",
    "{context}\n",
    "---\n",
    "\n",
    "We have asked the following question to the student for total_marks={total_marks}:\n",
    "\n",
    "---\n",
    "{question}\n",
    "---\n",
    "\n",
    "The student has responded with the following answer:\n",
    "\n",
    "---\n",
    "{student_answer}\n",
    "---\"\"\"\n",
    "        return user_prompt, system_prompt\n",
    "\n",
    "    @weave.op()\n",
    "    def grade_answer(\n",
    "        self, question: str, student_answer: str, total_marks: int\n",
    "    ) -> GradeExtractor:\n",
    "        user_prompt, system_prompt = self.get_prompt_for_grading(\n",
    "            question, student_answer, total_marks\n",
    "        )\n",
    "        return self._instructor_groq_client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": system_prompt,\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": user_prompt,\n",
    "                },\n",
    "            ],\n",
    "            model=self.model,\n",
    "            response_model=GradeExtractor,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we attempt to grade the response generated by the `EnglishStudentResponseAssistant` by the `EnglishGradingAssistant`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grading_assistant = EnglishGradingAssistant()\n",
    "\n",
    "rich.print(\n",
    "    grading_assistant.grade_answer(\n",
    "        question=query, student_answer=ideal_student_response, total_marks=5\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A trace for `EnglishGradingAssistant.grade_answer` showing the versioned `EnglishGradingAssistant` model object and the `GradeExtractor` object as its output which respresents the holistic grading of the student's answer in a structured manner.\n",
    "\n",
    "![](../images/weave_dashboard_grading_response.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
