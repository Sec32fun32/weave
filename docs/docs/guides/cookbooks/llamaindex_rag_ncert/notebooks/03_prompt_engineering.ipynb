{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU rich\n",
    "!pip install -qU wandb\n",
    "!pip install -qU git+https://github.com/wandb/weave.git@feat/groq\n",
    "!pip install -qU llama-index groq\n",
    "!pip install -qU llama-index-embeddings-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import rich\n",
    "import wandb\n",
    "import weave\n",
    "from google.colab import userdata\n",
    "\n",
    "import instructor\n",
    "from groq import Groq\n",
    "from pydantic import BaseModel\n",
    "from llama_index.core import (\n",
    "    ServiceContext, StorageContext, load_index_from_storage\n",
    ")\n",
    "from llama_index.core.base.base_retriever import BaseRetriever\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weave.init(project_name=\"groq-rag\")\n",
    "\n",
    "artifact = wandb.Api().artifact(\n",
    "    \"geekyrakshit/groq-rag/ncert-flamingoes-prose-embeddings:latest\"\n",
    ")\n",
    "artifact_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    embed_model=embed_model, llm=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context = StorageContext.from_defaults(persist_dir=artifact_dir)\n",
    "index = load_index_from_storage(\n",
    "    storage_context, service_context=service_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retreival_engine = index.as_retriever(\n",
    "    service_context=service_context,\n",
    "    similarity_top_k=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"what was the mood in the classroom when M. Hamel gave his last French lesson?\"\"\"\n",
    "response = retreival_engine.retrieve(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapter_name = response[0].node.metadata[\"file_name\"].split(\".\")[0].replace(\"_\", \" \").title()\n",
    "context = response[0].node.text\n",
    "\n",
    "rich.print(f\"{chapter_name=}\")\n",
    "rich.print(f\"{context=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnglishDoubtClearningAssistant(weave.Model):\n",
    "    model: str = \"llama3-8b-8192\"\n",
    "    _groq_client: Optional[Groq] = None\n",
    "    \n",
    "    def __init__(self, model: Optional[str] = None):\n",
    "        super().__init__()\n",
    "        self.model = model if model is not None else self.model\n",
    "        self._groq_client = Groq(\n",
    "            api_key=os.environ.get(\"GROQ_API_KEY\")\n",
    "        )\n",
    "    \n",
    "    @weave.op()\n",
    "    def predict(self, question: str, context: str) -> Tuple[str, str]:\n",
    "        chat_completion = self._groq_client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"\"\"\n",
    "You are a student in a class and your teacher has asked you to answer the following question.\n",
    "You have to write the answer in the given word limit.\"\"\",\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"\"\"\n",
    "We have provided context information below. \n",
    "\n",
    "---\n",
    "{context}\n",
    "---\n",
    "\n",
    "Answer the following question within 50-150 words:\n",
    "\n",
    "```\n",
    "{query}\n",
    "```\"\"\",\n",
    "                },\n",
    "            ],\n",
    "            model=self.model,\n",
    "        )\n",
    "        return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = EnglishDoubtClearningAssistant()\n",
    "\n",
    "rich.print(assistant.predict(question=query, context=context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnglishStudentResponseAssistant(weave.Model):\n",
    "    model: str = \"llama3-8b-8192\"\n",
    "    _groq_client: Optional[Groq] = None\n",
    "    \n",
    "    def __init__(self, model: Optional[str] = None):\n",
    "        super().__init__()\n",
    "        self.model = model if model is not None else self.model\n",
    "        self._groq_client = Groq(\n",
    "            api_key=os.environ.get(\"GROQ_API_KEY\")\n",
    "        )\n",
    "    \n",
    "    @weave.op()\n",
    "    def get_prompt(\n",
    "        self, question: str, context: str, word_limit_min: int, word_limit_max: int\n",
    "    ) -> Tuple[str, str]:\n",
    "        system_prompt = \"\"\"\n",
    "You are a student in a class and your teacher has asked you to answer the following question.\n",
    "You have to write the answer in the given word limit.\"\"\"\n",
    "        user_prompt = f\"\"\"\n",
    "We have provided context information below. \n",
    "\n",
    "---\n",
    "{context}\n",
    "---\n",
    "\n",
    "Answer the following question within {word_limit_min}-{word_limit_max} words:\n",
    "\n",
    "---\n",
    "{question}\n",
    "---\"\"\"\n",
    "        return system_prompt, user_prompt\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, question: str, total_marks: int) -> str:\n",
    "        response = retreival_engine.retrieve(question)\n",
    "        context = response[0].node.text\n",
    "        if total_marks < 3:\n",
    "            word_limit_min = 5\n",
    "            word_limit_max = 50\n",
    "        elif total_marks < 5:\n",
    "            word_limit_min = 50\n",
    "            word_limit_max = 100\n",
    "        else:\n",
    "            word_limit_min = 100\n",
    "            word_limit_max = 200\n",
    "        system_prompt, user_prompt = self.get_prompt(\n",
    "            question, context, word_limit_min, word_limit_max\n",
    "        )\n",
    "        chat_completion = self._groq_client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": system_prompt,\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": user_prompt,\n",
    "                },\n",
    "            ],\n",
    "            model=self.model,\n",
    "        )\n",
    "        return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assistant = EnglishDoubtClearningAssistant()\n",
    "\n",
    "rich.print(assistant.predict(question=query, context=context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnglishStudentResponseAssistant(weave.Model):\n",
    "    model: str = \"llama3-8b-8192\"\n",
    "    _groq_client: Optional[Groq] = None\n",
    "    \n",
    "    def __init__(self, model: Optional[str] = None):\n",
    "        super().__init__()\n",
    "        self.model = model if model is not None else self.model\n",
    "        self._groq_client = Groq(\n",
    "            api_key=os.environ.get(\"GROQ_API_KEY\")\n",
    "        )\n",
    "    \n",
    "    @weave.op()\n",
    "    def get_prompt(\n",
    "        self, question: str, context: str, word_limit_min: int, word_limit_max: int\n",
    "    ) -> Tuple[str, str]:\n",
    "        system_prompt = \"\"\"\n",
    "You are a student in a class and your teacher has asked you to answer the following question.\n",
    "You have to write the answer in the given word limit.\"\"\"\n",
    "        user_prompt = f\"\"\"\n",
    "We have provided context information below. \n",
    "\n",
    "---\n",
    "{context}\n",
    "---\n",
    "\n",
    "Answer the following question within {word_limit_min}-{word_limit_max} words:\n",
    "\n",
    "---\n",
    "{question}\n",
    "---\"\"\"\n",
    "        return system_prompt, user_prompt\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, question: str, total_marks: int) -> str:\n",
    "        response = retreival_engine.retrieve(question)\n",
    "        context = response[0].node.text\n",
    "        if total_marks < 3:\n",
    "            word_limit_min = 5\n",
    "            word_limit_max = 50\n",
    "        elif total_marks < 5:\n",
    "            word_limit_min = 50\n",
    "            word_limit_max = 100\n",
    "        else:\n",
    "            word_limit_min = 100\n",
    "            word_limit_max = 200\n",
    "        system_prompt, user_prompt = self.get_prompt(\n",
    "            question, context, word_limit_min, word_limit_max\n",
    "        )\n",
    "        chat_completion = self._groq_client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": system_prompt,\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": user_prompt,\n",
    "                },\n",
    "            ],\n",
    "            model=self.model,\n",
    "        )\n",
    "        return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = EnglishStudentResponseAssistant()\n",
    "\n",
    "ideal_student_response = assistant.predict(question=query, total_marks=5)\n",
    "rich.print(ideal_student_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradeExtractor(BaseModel):\n",
    "    question: str\n",
    "    student_answer: str\n",
    "    marks: float\n",
    "    total_marks: float\n",
    "    feedback: str\n",
    "\n",
    "\n",
    "class EnglishGradingAssistant(EnglishStudentResponseAssistant):\n",
    "    model: str = \"llama3-8b-8192\"\n",
    "    _groq_client: Optional[Groq] = None\n",
    "    _instructor_groq_client: Optional[instructor.Instructor] = None\n",
    "\n",
    "    def __init__(self, model: Optional[str] = None):\n",
    "        super().__init__(model=model)\n",
    "        self.model = model if model is not None else self.model\n",
    "        self._instructor_groq_client = instructor.from_groq(\n",
    "            Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
    "        )\n",
    "    \n",
    "    @weave.op()\n",
    "    def get_prompt_for_grading(\n",
    "        self,\n",
    "        question: str,\n",
    "        context: str,\n",
    "        total_marks: int,\n",
    "        student_answer: Optional[str] = None,\n",
    "    ) -> Tuple[str, str]:\n",
    "        system_prompt = \"\"\"\n",
    "You are a helpful assistant to an English teacher meant to grade the answer given by a student to a question.\n",
    "You have to extract the question , the student's answer, the marks awarded to the student out of total marks,\n",
    "the total marks and a contructive feedback to the student's answer with regards to how accurate it is with\n",
    "respect to the context.\n",
    "        \"\"\"\n",
    "        student_answer = (\n",
    "            self.predict(question, total_marks)\n",
    "            if student_answer is None\n",
    "            else student_answer\n",
    "        )\n",
    "        user_prompt = f\"\"\"\n",
    "We have provided context information below. \n",
    "\n",
    "---\n",
    "{context}\n",
    "---\n",
    "\n",
    "We have asked the following question to the student for total_marks={total_marks}:\n",
    "\n",
    "---\n",
    "{question}\n",
    "---\n",
    "\n",
    "The student has responded with the following answer:\n",
    "\n",
    "---\n",
    "{student_answer}\n",
    "---\"\"\"\n",
    "        return user_prompt, system_prompt\n",
    "    \n",
    "    @weave.op()\n",
    "    def grade_answer(\n",
    "        self, question: str, student_answer: str, total_marks: int\n",
    "    ) -> GradeExtractor:\n",
    "        user_prompt, system_prompt = self.get_prompt_for_grading(\n",
    "            question, student_answer, total_marks\n",
    "        )\n",
    "        return self._instructor_groq_client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": system_prompt,\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": user_prompt,\n",
    "                },\n",
    "            ],\n",
    "            model=self.model,\n",
    "            response_model=GradeExtractor,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = EnglishGradingAssistant()\n",
    "\n",
    "rich.print(assistant.grade_answer(question=query, student_answer=ideal_student_response, total_marks=5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
