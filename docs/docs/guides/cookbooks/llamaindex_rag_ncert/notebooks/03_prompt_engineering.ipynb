{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU rich\n",
    "!pip install -qU wandb\n",
    "!pip install -qU git+https://github.com/wandb/weave.git@feat/groq\n",
    "!pip install -qU llama-index groq\n",
    "!pip install -qU llama-index-embeddings-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import rich\n",
    "import wandb\n",
    "import weave\n",
    "from google.colab import userdata\n",
    "\n",
    "from groq import Groq\n",
    "from llama_index.core import (\n",
    "    ServiceContext, StorageContext, load_index_from_storage\n",
    ")\n",
    "from llama_index.core.base.base_retriever import BaseRetriever\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weave.init(project_name=\"groq-rag\")\n",
    "\n",
    "artifact = wandb.Api().artifact(\n",
    "    \"geekyrakshit/groq-rag/ncert-flamingoes-prose-embeddings:latest\"\n",
    ")\n",
    "artifact_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    embed_model=embed_model, llm=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context = StorageContext.from_defaults(persist_dir=artifact_dir)\n",
    "index = load_index_from_storage(\n",
    "    storage_context, service_context=service_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retreival_engine = index.as_retriever(\n",
    "    service_context=service_context,\n",
    "    similarity_top_k=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"what was the mood in the classroom when M. Hamel gave his last French lesson?\"\"\"\n",
    "response = retreival_engine.retrieve(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapter_name = response[0].node.metadata[\"file_name\"].split(\".\")[0].replace(\"_\", \" \").title()\n",
    "context = response[0].node.text\n",
    "\n",
    "rich.print(f\"{chapter_name=}\")\n",
    "rich.print(f\"{context=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnglishDoubtClearningAssistant(weave.Model):\n",
    "    model: str = \"llama3-8b-8192\"\n",
    "    _groq_client: Optional[Groq] = None\n",
    "    \n",
    "    def __init__(self, model: Optional[str] = None):\n",
    "        super().__init__()\n",
    "        self.model = model if model is not None else self.model\n",
    "        self._groq_client = Groq(\n",
    "            api_key=os.environ.get(\"GROQ_API_KEY\")\n",
    "        )\n",
    "    \n",
    "    @weave.op()\n",
    "    def predict(self, question: str, context: str) -> Tuple[str, str]:\n",
    "        chat_completion = self._groq_client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"\"\"\n",
    "You are a student in a class and your teacher has asked you to answer the following question.\n",
    "You have to write the answer in the given word limit.\"\"\",\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"\"\"\n",
    "We have provided context information below. \n",
    "\n",
    "---\n",
    "{context}\n",
    "---\n",
    "\n",
    "Answer the following question within 50-150 words:\n",
    "\n",
    "```\n",
    "{query}\n",
    "```\"\"\",\n",
    "                },\n",
    "            ],\n",
    "            model=self.model,\n",
    "        )\n",
    "        return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = EnglishDoubtClearningAssistant()\n",
    "\n",
    "rich.print(assistant.predict(question=query, context=context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnglishTeachingAssistant(weave.Model):\n",
    "    embeddings_artifact_wandb_address: Optional[str] = None\n",
    "    embedding_model_huggingface_address: Optional[str] = None\n",
    "    _groq_client: Optional[Groq] = None\n",
    "    _retreival_engine: Optional[BaseRetriever] = None\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embeddings_artifact_wandb_address: Optional[\n",
    "            str\n",
    "        ] = \"geekyrakshit/groq-rag/ncert-flamingoes-prose-embeddings:latest\",\n",
    "        embedding_model_huggingface_address: Optional[str] = \"BAAI/bge-small-en-v1.5\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embeddings_artifact_wandb_address = embeddings_artifact_wandb_address\n",
    "        self.embedding_model_huggingface_address = embedding_model_huggingface_address\n",
    "        self._build_vector_store()\n",
    "        self._groq_client = Groq(\n",
    "            api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    "        )\n",
    "\n",
    "    def _build_vector_store(self):\n",
    "        artifact = wandb.Api().artifact(\n",
    "            \"geekyrakshit/groq-rag/ncert-flamingoes-prose-embeddings:latest\"\n",
    "        )\n",
    "        artifact_dir = artifact.download()\n",
    "        service_context = ServiceContext.from_defaults(\n",
    "            embed_model=HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\"),\n",
    "            llm=None,\n",
    "        )\n",
    "        storage_context = StorageContext.from_defaults(persist_dir=artifact_dir)\n",
    "        index = load_index_from_storage(\n",
    "            storage_context, service_context=service_context\n",
    "        )\n",
    "        self._retreival_engine = index.as_retriever(\n",
    "            service_context=service_context,\n",
    "            similarity_top_k=10,\n",
    "        )\n",
    "\n",
    "    @weave.op()\n",
    "    def get_response_from_groq(self, system_prompt: str, user_prompt: str) -> str:\n",
    "        chat_completion = self._groq_client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": system_prompt,\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": user_prompt,\n",
    "                },\n",
    "            ],\n",
    "            model=\"llama3-8b-8192\",\n",
    "        )\n",
    "        return chat_completion.choices[0].message.content\n",
    "\n",
    "    @weave.op()\n",
    "    def get_prompt_for_emulating_student_response(\n",
    "        self, question: str, context: str, word_limit_min: int, word_limit_max: int\n",
    "    ) -> Tuple[str, str]:\n",
    "        user_prompt = f\"\"\"\n",
    "        We have provided context information below. \n",
    "\n",
    "        ---\n",
    "        {context}\n",
    "        ---\n",
    "\n",
    "        Answer the following question within {word_limit_min}-{word_limit_max} words:\n",
    "\n",
    "        ```\n",
    "        {question}\n",
    "        ```\n",
    "        \"\"\"\n",
    "        system_prompt = \"\"\"\n",
    "        You are a student in a class and your teacher has asked you to answer the following question.\n",
    "        You have to write the answer in the given word limit.\n",
    "        \"\"\"\n",
    "        return user_prompt, system_prompt\n",
    "\n",
    "    @weave.op()\n",
    "    def get_prompt_for_doubt_clearning(\n",
    "        self, question: str, context: str\n",
    "    ) -> Tuple[str, str]:\n",
    "        user_prompt = f\"\"\"\n",
    "        We have provided context information below. \n",
    "\n",
    "        ---\n",
    "        {context}\n",
    "        ---\n",
    "\n",
    "        Answer the following question as helpfully as you can:\n",
    "\n",
    "        ```\n",
    "        {question}\n",
    "        ```\n",
    "        \"\"\"\n",
    "        system_prompt = \"\"\"\n",
    "        You are a helpful assistant to an English teacher meant to solve the doubts of students.\n",
    "        You have to write the answer to the following question as helpfully as you can.\n",
    "        \"\"\"\n",
    "        return user_prompt, system_prompt\n",
    "\n",
    "    @weave.op()\n",
    "    def format_doubt_clearning_response(\n",
    "        self, answer: str, context: str, chapter_name: str\n",
    "    ) -> str:\n",
    "        return f\"\"\"\n",
    "        The answer is:\n",
    "        \n",
    "        {answer}\n",
    "        \n",
    "        The answer is based on the following excerpt from the chapter {chapter_name} from the NCERT Flamingo textbook:\n",
    "\n",
    "        {context}\n",
    "        \"\"\"\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(\n",
    "        self,\n",
    "        question: str,\n",
    "        total_marks: Optional[int] = None,\n",
    "        grade_response: Optional[bool] = False,\n",
    "    ) -> str:\n",
    "        response = self._retreival_engine.retrieve(question)\n",
    "        chapter_name = (\n",
    "            response[0]\n",
    "            .node.metadata[\"file_name\"]\n",
    "            .split(\".\")[0]\n",
    "            .replace(\"_\", \" \")\n",
    "            .title()\n",
    "        )\n",
    "        context = response[0].node.text\n",
    "        if total_marks:\n",
    "            if total_marks < 3:\n",
    "                word_limit_min = 5\n",
    "                word_limit_max = 50\n",
    "            elif total_marks < 5:\n",
    "                word_limit_min = 50\n",
    "                word_limit_max = 100\n",
    "            else:\n",
    "                word_limit_min = 100\n",
    "                word_limit_max = 200\n",
    "        else:\n",
    "            word_limit_min = None\n",
    "            word_limit_max = None\n",
    "\n",
    "        if total_marks:\n",
    "            if not grade_response:\n",
    "                (\n",
    "                    user_prompt,\n",
    "                    system_prompt,\n",
    "                ) = self.get_prompt_for_emulating_student_response(\n",
    "                    question, context, word_limit_min, word_limit_max\n",
    "                )\n",
    "                return self.get_response_from_groq(\n",
    "                    system_prompt=system_prompt, user_prompt=user_prompt\n",
    "                )\n",
    "\n",
    "        user_prompt, system_prompt = self.get_prompt_for_doubt_clearning(\n",
    "            question, context\n",
    "        )\n",
    "        answer = self.get_response_from_groq(\n",
    "            system_prompt=system_prompt, user_prompt=user_prompt\n",
    "        )\n",
    "        return self.format_doubt_clearning_response(answer, context, chapter_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = EnglishTeachingAssistant()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rich.print(assistant.predict(question=query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rich.print(assistant.predict(question=query, marks=5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
