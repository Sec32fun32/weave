{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict, Optional, Tuple\n",
    "\n",
    "import wandb\n",
    "import weave\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "import instructor\n",
    "from openai import OpenAI\n",
    "from groq import Groq\n",
    "from pydantic import BaseModel\n",
    "from llama_index.core import (\n",
    "    ServiceContext, StorageContext, load_index_from_storage\n",
    ")\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weave.init(project_name=\"groq-rag\")\n",
    "\n",
    "artifact = wandb.Api().artifact(\n",
    "    \"geekyrakshit/groq-rag/ncert-flamingoes-prose-embeddings:latest\"\n",
    ")\n",
    "artifact_dir = artifact.download()\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    embed_model=embed_model, llm=None\n",
    ")\n",
    "\n",
    "storage_context = StorageContext.from_defaults(persist_dir=artifact_dir)\n",
    "index = load_index_from_storage(\n",
    "    storage_context, service_context=service_context\n",
    ")\n",
    "retreival_engine = index.as_retriever(\n",
    "    service_context=service_context,\n",
    "    similarity_top_k=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnglishStudentResponseAssistant(weave.Model):\n",
    "    model: str = \"llama3-8b-8192\"\n",
    "    _groq_client: Optional[Groq] = None\n",
    "\n",
    "    def __init__(self, model: Optional[str] = None):\n",
    "        super().__init__()\n",
    "        self.model = model if model is not None else self.model\n",
    "        self._groq_client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
    "\n",
    "    @weave.op()\n",
    "    def get_prompt(\n",
    "        self, question: str, context: str, word_limit_min: int, word_limit_max: int\n",
    "    ) -> Tuple[str, str]:\n",
    "        system_prompt = \"\"\"\n",
    "You are a student in a class and your teacher has asked you to answer the following question.\n",
    "You have to write the answer in the given word limit.\"\"\"\n",
    "        user_prompt = f\"\"\"\n",
    "We have provided context information below. \n",
    "\n",
    "---\n",
    "{context}\n",
    "---\n",
    "\n",
    "Answer the following question within {word_limit_min}-{word_limit_max} words:\n",
    "\n",
    "---\n",
    "{question}\n",
    "---\"\"\"\n",
    "        return system_prompt, user_prompt\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, question: str, total_marks: int) -> str:\n",
    "        response = retreival_engine.retrieve(question)\n",
    "        context = response[0].node.text\n",
    "        if total_marks < 3:\n",
    "            word_limit_min = 5\n",
    "            word_limit_max = 50\n",
    "        elif total_marks < 5:\n",
    "            word_limit_min = 50\n",
    "            word_limit_max = 100\n",
    "        else:\n",
    "            word_limit_min = 100\n",
    "            word_limit_max = 200\n",
    "        system_prompt, user_prompt = self.get_prompt(\n",
    "            question, context, word_limit_min, word_limit_max\n",
    "        )\n",
    "        chat_completion = self._groq_client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": system_prompt,\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": user_prompt,\n",
    "                },\n",
    "            ],\n",
    "            model=self.model,\n",
    "        )\n",
    "        return {\n",
    "            \"response\": chat_completion.choices[0].message.content,\n",
    "            \"context\": context,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JudgeResponse(BaseModel):\n",
    "    question: str\n",
    "    ground_truth_answer: str\n",
    "    assistant_answer: str\n",
    "    marks: float\n",
    "    total_marks: int\n",
    "    explanation: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenaAIJudgeModel(weave.Model):\n",
    "    model: str = \"gpt-4\"\n",
    "    _openai_client: Optional[instructor.Instructor] = None\n",
    "\n",
    "    def __init__(self, model: Optional[str] = None):\n",
    "        super().__init__()\n",
    "        self.model = model if model is not None else self.model\n",
    "        self._openai_client = instructor.from_openai(\n",
    "            OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "        )\n",
    "\n",
    "    @weave.op()\n",
    "    def compose_judgement(\n",
    "        self,\n",
    "        question: str,\n",
    "        context: str,\n",
    "        ground_truth_answer: str,\n",
    "        assistant_answer: str,\n",
    "        total_marks: int,\n",
    "    ) -> JudgeResponse:\n",
    "        system_prompt = f\"\"\"\n",
    "You are an expert in teacher of English langugage and literature.\n",
    "Given a question, a context, a ground truth answer and an answer from an AI assistant,\n",
    "you have to judge the assistant's answer based on the following criteria and assign\n",
    "a score between 0 and total marks:\n",
    "\n",
    "1. how well the assistant answers the question with respect to the context.\n",
    "2. how well the assistant's answer holds up in correctness and relevance to\n",
    "    the ground truth answer (assuming the ground truth answer is perfect).\n",
    "\n",
    "You have to extract the question, the ground truth answer, the assistant's answer,\n",
    "the marks to be awarded to the assistant's answer, the total marks for the question,\n",
    "and a detailed explanation as to how the assistant's answer was judged.\"\"\"\n",
    "        user_prompt = f\"\"\"\n",
    "We have asked the following question to an AI assistant for total_marks={total_marks}:\n",
    "\n",
    "---\n",
    "{question}\n",
    "---\n",
    "\n",
    "We have provided context information below. \n",
    "\n",
    "---\n",
    "{context}\n",
    "---\n",
    "\n",
    "Th AI assistant has responded with the following answer:\n",
    "\n",
    "---\n",
    "{assistant_answer}\n",
    "---\n",
    "\n",
    "An ideal answer to the question would be the following:\n",
    "\n",
    "---\n",
    "{ground_truth_answer}\n",
    "---\"\"\"\n",
    "        return self._openai_client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": system_prompt,\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": user_prompt,\n",
    "                },\n",
    "            ],\n",
    "            model=self.model,\n",
    "            response_model=JudgeResponse,\n",
    "        )\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(\n",
    "        self,\n",
    "        question: str,\n",
    "        answer: str,\n",
    "        marks: int,\n",
    "        model_output: Dict[str, str],\n",
    "    ) -> Dict[str, float]:\n",
    "        if marks == \"3-4\":\n",
    "            total_marks = 4\n",
    "        elif marks == \"5-6\":\n",
    "            total_marks = 6\n",
    "        judge_response = self.compose_judgement(\n",
    "            question=question,\n",
    "            context=model_output[\"context\"],\n",
    "            ground_truth_answer=answer,\n",
    "            assistant_answer=model_output[\"response\"],\n",
    "            total_marks=total_marks,\n",
    "        )\n",
    "        return {\n",
    "            \"marks\": judge_response.marks,\n",
    "            \"fractional_marks\": judge_response.marks / total_marks,\n",
    "            \"percentage\": (judge_response.marks / total_marks) * 100,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = EnglishStudentResponseAssistant()\n",
    "\n",
    "\n",
    "@weave.op()\n",
    "def get_assistant_prediction(question: str, marks: str):\n",
    "    if marks == \"3-4\":\n",
    "        marks = 4\n",
    "    elif marks == \"5-6\":\n",
    "        marks = 6\n",
    "    return assistant.predict(question, marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_model = OpenaAIJudgeModel()\n",
    "dataset = weave.ref('flamingos-prose-question-bank:v1').get()\n",
    "evaluation = weave.Evaluation(dataset=dataset, scorers=[judge_model.predict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await evaluation.evaluate(get_assistant_prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "groq2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
