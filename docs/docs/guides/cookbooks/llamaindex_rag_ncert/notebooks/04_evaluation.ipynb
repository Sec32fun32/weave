{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an Evaluation Pipeline\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/wandb/weave/blob/master/docs/docs/guides/cookbooks/llamaindex_rag_ncert/notebooks/04_evaluation.ipynb)\n",
    "\n",
    "To iterate on any AI application, we need a way to systematically evaluate its performace to check if it's improving or not. To do so, a common practice is to test it against the same set of examples when there is a change. In this recipe, we will build an evaluation pipeline to evaluate the responses of our AI assistant using [`weave.Evaluation`](https://wandb.github.io/weave/guides/core-types/evaluations) which is a flexible API that provides us with a first-class way to track evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the dependencies\n",
    "\n",
    "First, let us install all the libraries that we would need to build the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU rich\n",
    "!pip install -U instructor\n",
    "!pip install -qU wandb\n",
    "!pip install -qU git+https://github.com/wandb/weave.git@feat/groq\n",
    "!pip install -qU llama-index groq\n",
    "!pip install -qU llama-index-embeddings-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "from typing import Dict, Optional, Tuple\n",
    "\n",
    "import instructor\n",
    "import wandb\n",
    "from groq import Groq\n",
    "from llama_index.core import ServiceContext, StorageContext, load_index_from_storage\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "import weave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Assistant\n",
    "\n",
    "In this recipe, we will demonstrate an evaluation strategy for the `EnglishStudentResponseAssistant`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weave.init(project_name=\"groq-rag\")\n",
    "\n",
    "artifact = wandb.Api().artifact(\n",
    "    \"geekyrakshit/groq-rag/ncert-flamingoes-prose-embeddings:latest\"\n",
    ")\n",
    "artifact_dir = artifact.download()\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=None)\n",
    "storage_context = StorageContext.from_defaults(persist_dir=artifact_dir)\n",
    "index = load_index_from_storage(storage_context, service_context=service_context)\n",
    "retreival_engine = index.as_retriever(\n",
    "    service_context=service_context,\n",
    "    similarity_top_k=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROQ_API_KEY = getpass(\"Enter your GROQ API key: \")\n",
    "OPENAI_API_KEY = getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnglishStudentResponseAssistant(weave.Model):\n",
    "    model: str = \"llama3-8b-8192\"\n",
    "    _groq_client: Optional[Groq] = None\n",
    "\n",
    "    def __init__(self, model: Optional[str] = None):\n",
    "        super().__init__()\n",
    "        self.model = model if model is not None else self.model\n",
    "        self._groq_client = Groq(api_key=GROQ_API_KEY)\n",
    "\n",
    "    @weave.op()\n",
    "    def get_prompt(\n",
    "        self, question: str, context: str, word_limit_min: int, word_limit_max: int\n",
    "    ) -> Tuple[str, str]:\n",
    "        system_prompt = \"\"\"\n",
    "You are a student in a class and your teacher has asked you to answer the following question.\n",
    "You have to write the answer in the given word limit.\"\"\"\n",
    "        user_prompt = f\"\"\"\n",
    "We have provided context information below.\n",
    "\n",
    "---\n",
    "{context}\n",
    "---\n",
    "\n",
    "Answer the following question within {word_limit_min}-{word_limit_max} words:\n",
    "\n",
    "---\n",
    "{question}\n",
    "---\"\"\"\n",
    "        return system_prompt, user_prompt\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, question: str, total_marks: int) -> str:\n",
    "        response = retreival_engine.retrieve(question)\n",
    "        context = response[0].node.text\n",
    "        if total_marks < 3:\n",
    "            word_limit_min = 5\n",
    "            word_limit_max = 50\n",
    "        elif total_marks < 5:\n",
    "            word_limit_min = 50\n",
    "            word_limit_max = 100\n",
    "        else:\n",
    "            word_limit_min = 100\n",
    "            word_limit_max = 200\n",
    "        system_prompt, user_prompt = self.get_prompt(\n",
    "            question, context, word_limit_min, word_limit_max\n",
    "        )\n",
    "        chat_completion = self._groq_client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": system_prompt,\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": user_prompt,\n",
    "                },\n",
    "            ],\n",
    "            model=self.model,\n",
    "        )\n",
    "        return {\n",
    "            \"response\": chat_completion.choices[0].message.content,\n",
    "            \"context\": context,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an Evaluation Dataset\n",
    "\n",
    "We built an evaluation dataset by scraping a question bank of solved question-answer pairs of the Flamigo textbook from [LearnCBSE](https://www.learncbse.in/chapter-wise-important-questions-class-12-english/). The dataset consists of 358 question-answer pairs corresponding to the 8 chapters from our knowledge base dataset. We log this dataset as a [`weave.Dataset`](https://wandb.github.io/weave/guides/core-types/datasets) which enables us to collect examples for evaluation and automatically track versions for accurate comparisons. The dataset consists of examples in the following format:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"question\": \"What was the mood in the classroom when M. Hamel gave his last French lesson? \",\n",
    "  \"answer\": \"When M.Hamel was giving his last French ; lesson, the mood in the classroom was solemn and sombre. When he announced that this was their last French lesson everyone present in the classroom suddenly developed patriotic feelings for their native language and genuinely regretted ignoring their mother tongue.\",\n",
    "  \"marks\": \"3-4\",\n",
    "  \"chapter_name\": \"The Last Lesson\"\n",
    "}\n",
    "```\n",
    "\n",
    "You can explore the evaluation dataset in the weave UI [here](https://wandb.ai/geekyrakshit/groq-rag/weave/objects/flamingos-prose-question-bank/versions/sfx9Qg4FYq4eOEBt5SjNdkhcjqXJaqRN4CGvlCQAFcU).\n",
    "\n",
    "![](../images/weave_evaluation_dataset.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating with an LLM Judge\n",
    "\n",
    "One approach to evaluate an LLM application is to use another LLM as a judge to evaluate aspects of it. In this recipe, we demonstrate a simple example of using an LLM judge as a `weave.Scorer` to try to measure the correctness of the AI assistant's response by prompting it to verify if the the response is relevant to the context and how well it holds up to the ground-truth answer from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JudgeResponse(BaseModel):\n",
    "    marks: float\n",
    "    explanation: str\n",
    "\n",
    "\n",
    "class OpenaAIJudgeModel(weave.Scorer):\n",
    "    model: str = \"gpt-4\"\n",
    "    max_retries: int = 5\n",
    "    _openai_client: Optional[instructor.Instructor] = None\n",
    "\n",
    "    def __init__(self, model: Optional[str] = None):\n",
    "        super().__init__()\n",
    "        self.model = model if model is not None else self.model\n",
    "        self._openai_client = instructor.from_openai(\n",
    "            OpenAI(api_key=OPENAI_API_KEY),\n",
    "            mode=instructor.Mode.TOOLS,\n",
    "        )\n",
    "\n",
    "    @weave.op()\n",
    "    def compose_judgement(\n",
    "        self,\n",
    "        question: str,\n",
    "        context: str,\n",
    "        ground_truth_answer: str,\n",
    "        assistant_answer: str,\n",
    "        total_marks: int,\n",
    "    ) -> JudgeResponse:\n",
    "        system_prompt = \"\"\"\n",
    "You are an expert in teacher of English langugage and literature.\n",
    "Given a question, a context, a ground truth answer and an answer from an AI assistant,\n",
    "you have to judge the assistant's answer based on the following criteria and assign\n",
    "a score between 0 and total marks:\n",
    "\n",
    "1. how well the assistant answers the question with respect to the context.\n",
    "2. how well the assistant's answer holds up in correctness and relevance to\n",
    "    the ground truth answer (assuming the ground truth answer is perfect).\n",
    "\n",
    "You have to extract the marks to be awarded to the assistant's answer and a detailed\n",
    "explanation as to how the assistant's answer was judged.\"\"\"\n",
    "        user_prompt = f\"\"\"\n",
    "We have asked the following question to an AI assistant for total marks of {total_marks}:\n",
    "\n",
    "---\n",
    "{question}\n",
    "---\n",
    "\n",
    "We have provided context information below.\n",
    "\n",
    "---\n",
    "{context}\n",
    "---\n",
    "\n",
    "Th AI assistant has responded with the following answer:\n",
    "\n",
    "---\n",
    "{assistant_answer}\n",
    "---\n",
    "\n",
    "An ideal answer to the question would be the following:\n",
    "\n",
    "---\n",
    "{ground_truth_answer}\n",
    "---\"\"\"\n",
    "        return self._openai_client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": system_prompt,\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": user_prompt,\n",
    "                },\n",
    "            ],\n",
    "            max_retries=self.max_retries,\n",
    "            model=self.model,\n",
    "            response_model=JudgeResponse,\n",
    "        )\n",
    "\n",
    "    @weave.op()\n",
    "    def score(\n",
    "        self,\n",
    "        question: str,\n",
    "        answer: str,\n",
    "        marks: str,\n",
    "        model_output: Dict[str, str],\n",
    "    ) -> Dict[str, float]:\n",
    "        if marks == \"3-4\":\n",
    "            total_marks = 4\n",
    "        elif marks == \"5-6\":\n",
    "            total_marks = 6\n",
    "        else:\n",
    "            total_marks = 4\n",
    "        judge_response = self.compose_judgement(\n",
    "            question=question,\n",
    "            context=model_output[\"context\"],\n",
    "            ground_truth_answer=answer,\n",
    "            assistant_answer=model_output[\"response\"],\n",
    "            total_marks=total_marks,\n",
    "        )\n",
    "        if not hasattr(judge_response, \"marks\"):\n",
    "            return {\"marks\": 0.0, \"fractional_marks\": 0.0, \"percentage\": 0.0}\n",
    "        return {\n",
    "            \"marks\": judge_response.marks,\n",
    "            \"fractional_marks\": judge_response.marks / total_marks,\n",
    "            \"percentage\": (judge_response.marks / total_marks) * 100,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating our LLM Application\n",
    "\n",
    "Finally, let us put everything and evaluate our LLM assistant using [`weave.Evaluation`](https://wandb.github.io/weave/guides/core-types/evaluations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = EnglishStudentResponseAssistant()\n",
    "\n",
    "\n",
    "@weave.op()\n",
    "async def get_assistant_prediction(question: str, marks: str):\n",
    "    if marks == \"3-4\":\n",
    "        total_marks = 4\n",
    "    elif marks == \"5-6\":\n",
    "        total_marks = 6\n",
    "    else:\n",
    "        total_marks = 4\n",
    "    return assistant.predict(question, total_marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = weave.ref(\"flamingos-prose-question-bank:v1\").get()\n",
    "evaluation = weave.Evaluation(dataset=dataset, scorers=[OpenaAIJudgeModel()])\n",
    "await evaluation.evaluate(get_assistant_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the [`weave.Evaluation`](https://wandb.github.io/weave/guides/core-types/evaluations) class, you can be sure you're comparing apples-to-apples by keeping track of all of the details that you're experimenting and evaluating with. Weave will take each example, pass it through your application and score the output on multiple custom scoring functions. By doing this, you'll have a view of the performance of your application, and a rich UI to drill into individual ouputs and scores.\n",
    "\n",
    "![](../images/weave_evaluation_dashboard.gif)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "groq2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
