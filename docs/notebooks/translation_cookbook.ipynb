{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](http://wandb.me/translation_cookbook)\n",
    "\n",
    "<!--- @wandbcode{translation-cookbook} -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation using Large Language Models\n",
    "\n",
    "Translating text accurately while preserving nuances and cultural context is a challenging task. This guide demonstrates how to implement a robust translation pipeline using Large Language Models (LLMs) and Weave, a powerful framework for building, tracking, and evaluating LLM applications. By combining the effectiveness of LLMs with Weave's robust tooling, you'll learn to create a translation pipeline that produces high-quality translations while gaining insights into the translation process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why use Weave for Translation?\n",
    "\n",
    "In this tutorial, we'll use Weave to implement and evaluate a translation pipeline for the OPUS-100 dataset. You'll learn how to:\n",
    "\n",
    "1. **Track your LLM pipeline**: Use Weave to automatically log inputs, outputs, and intermediate steps of your translation process.\n",
    "2. **Evaluate LLM outputs**: Create rigorous, apples-to-apples evaluations of your translations using Weave's built-in tools.\n",
    "3. **Build composable operations**: Combine and reuse Weave operations across different parts of your translation pipeline.\n",
    "4. **Integrate seamlessly**: Add Weave to your existing Python code with minimal overhead.\n",
    "\n",
    "By the end of this tutorial, you'll have created a translation pipeline that leverages Weave's capabilities for model serving, evaluation, and result tracking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the environment\n",
    "\n",
    "First, let's set up our environment and import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU weave litellm datasets transformers pydantic set-env-colab-kaggle-dotenv instructor python-Levenshtein nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weave\n",
    "from pydantic import BaseModel, Field\n",
    "from set_env import set_env\n",
    "from datasets import load_dataset\n",
    "import litellm\n",
    "import instructor\n",
    "from instructor import Mode\n",
    "from typing import List\n",
    "import sacrebleu\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "\n",
    "set_env(\"WANDB_API_KEY\")\n",
    "set_env(\"OPENAI_API_KEY\")\n",
    "\n",
    "print(\"Weave version:\", weave.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weave.init(\"translation-cookbook\")\n",
    "# Patch OpenAI client with Instructor\n",
    "client = instructor.from_litellm(litellm.completion, mode=Mode.JSON)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're using Weave to track our experiment and LiteLLM with Instructor for text generation. The `weave.init()` call sets up a new Weave project for our translation task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the TranslationPair model\n",
    "\n",
    "We'll create a simple `TranslationPair` class to represent our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define TranslationPair model\n",
    "class TranslationPair(BaseModel):\n",
    "    source: str\n",
    "    target: str\n",
    "    target_language: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class encapsulates the source text, target (reference) translation, and target language, which will be the input to our translation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a small sample of the OPUS-100 dataset\n",
    "dataset = load_dataset(\"opus100\", \"en-fr\", split=\"test[:5]\")\n",
    "\n",
    "# Create sample TranslationPair\n",
    "sample_pair = TranslationPair(\n",
    "    source=dataset[0]['translation']['en'],\n",
    "    target=dataset[0]['translation']['fr'],\n",
    "    target_language=\"French\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the translation pipeline\n",
    "\n",
    "Now, let's implement the core translation logic using Weave operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define structured output for translation\n",
    "class Translation(BaseModel):\n",
    "    translation: str\n",
    "\n",
    "# Define structured output for translation evaluation\n",
    "class TranslationEvaluation(BaseModel):\n",
    "    adequacy: int = Field(..., ge=1, le=5)\n",
    "    adequacy_explanation: str\n",
    "    fluency: int = Field(..., ge=1, le=5)\n",
    "    fluency_explanation: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def translate_text(text: str, target_language: str, model: str = \"gpt-3.5-turbo\") -> str:\n",
    "    prompt = f\"\"\"You are a highly skilled professional translator with expertise in both the source language (English) and the target language ({target_language}). Translate the following English text into {target_language} with accuracy, fluency, and cultural appropriateness.\n",
    "\n",
    "Instructions:\n",
    "1. Maintain the original meaning and tone of the text.\n",
    "2. Use natural and idiomatic expressions in the target language.\n",
    "3. Preserve any specialized terminology or proper nouns.\n",
    "4. Adapt cultural references when necessary for the target audience.\n",
    "5. Ensure grammatical correctness and appropriate style for the target language.\n",
    "6. Do not add any explanations or comments to the translation.\n",
    "7. Translate acronyms only if they have a standard equivalent in the target language.\n",
    "\n",
    "Source text (English):\n",
    "{text}\n",
    "\n",
    "Provide only the {target_language} translation below:\n",
    "\"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        response_model=Translation,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    \n",
    "    return response.translation\n",
    "\n",
    "class TranslationPipeline(weave.Model):\n",
    "    model: str = \"gpt-3.5-turbo\"\n",
    "    \n",
    "    @weave.op()\n",
    "    def predict(self, pair: TranslationPair) -> dict:\n",
    "        translation = translate_text(pair[\"source\"], pair[\"target_language\"], self.model)\n",
    "        return {\"translation\": translation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using `@weave.op()` decorators, we ensure that Weave tracks the inputs, outputs, and execution of these functions. This allows us to monitor the progress of our translation pipeline and evaluate its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Weave Dataset and run evaluation\n",
    "\n",
    "To evaluate our pipeline, we'll create a Weave Dataset and run an evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Weave Dataset\n",
    "weave_dataset = weave.Dataset(\n",
    "    name=\"opus100_en_fr_sample\",\n",
    "    rows=[\n",
    "        {\n",
    "            \"pair\": TranslationPair(\n",
    "                source=item['translation']['en'],\n",
    "                target=item['translation']['fr'],\n",
    "                target_language=\"French\"\n",
    "            )\n",
    "        }\n",
    "        for item in dataset\n",
    "    ]\n",
    ")\n",
    "\n",
    "weave.publish(weave_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement evaluation metrics\n",
    "\n",
    "To assess the quality of our translations, we'll implement both LLM-based and traditional metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def llm_evaluate_translation(source: str, reference: str, hypothesis: str, target_language: str, model: str = \"gpt-4o\") -> TranslationEvaluation:\n",
    "    prompt = f\"\"\"\n",
    "    Source (English): {source}\n",
    "    Reference ({target_language}): {reference}\n",
    "    Hypothesis ({target_language}): {hypothesis}\n",
    "\n",
    "    Evaluate the translation based on the following criteria:\n",
    "    1. Adequacy (1-5): How well does the translation convey the meaning of the source text?\n",
    "       - Consider accuracy of information, preservation of nuances, and completeness.\n",
    "    2. Fluency (1-5): How natural and fluent is the translation in the target language?\n",
    "       - Consider grammar, word choice, idiomatic expressions, and overall readability.\n",
    "\n",
    "    Additional aspects to consider:\n",
    "    - Terminology consistency\n",
    "    - Cultural appropriateness\n",
    "    - Handling of proper nouns and acronyms\n",
    "    - Preservation of tone and style\n",
    "\n",
    "    Provide a score (1-5) and a brief explanation (max 50 words) for each criterion.\n",
    "    \"\"\"\n",
    "        \n",
    "    evaluation = client.chat.completions.create(\n",
    "        model=model,\n",
    "        response_model=TranslationEvaluation,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    \n",
    "    return evaluation\n",
    "\n",
    "\n",
    "# Define the scorer function\n",
    "@weave.op()\n",
    "def llm_translation_quality_scorer(pair: TranslationPair, model_output: dict) -> dict:\n",
    "    evaluation = llm_evaluate_translation(\n",
    "        pair[\"source\"],\n",
    "        pair[\"target\"],\n",
    "        model_output[\"translation\"],\n",
    "        pair[\"target_language\"]\n",
    "    )\n",
    "    return {\n",
    "        \"adequacy\": evaluation.adequacy,\n",
    "        \"fluency\": evaluation.fluency,\n",
    "        \"adequacy_explanation\": evaluation.adequacy_explanation,\n",
    "        \"fluency_explanation\": evaluation.fluency_explanation\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def calculate_translation_metrics(reference: str, hypothesis: str) -> dict:\n",
    "    # BLEU score\n",
    "    bleu = sacrebleu.corpus_bleu([hypothesis], [[reference]])\n",
    "    \n",
    "    # Character Error Rate (CER)\n",
    "    cer = levenshtein_distance(reference, hypothesis) / max(len(reference), len(hypothesis))\n",
    "    \n",
    "    # Translation Edit Rate (TER)\n",
    "    ter = sacrebleu.sentence_ter(hypothesis, [reference])\n",
    "    \n",
    "    return {\n",
    "        \"bleu\": bleu.score,\n",
    "        \"cer\": cer,\n",
    "        \"ter\": ter.score\n",
    "    }\n",
    "\n",
    "@weave.op()\n",
    "def calculate_translation_metrics_scorer(pair: TranslationPair, model_output: dict) -> dict:\n",
    "    evaluation = calculate_translation_metrics(pair[\"source\"], model_output[\"translation\"])\n",
    "    return evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These evaluation functions use both an LLM and traditional metrics to assess the quality of the generated translations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "evaluation = weave.Evaluation(dataset=weave_dataset, scorers=[llm_translation_quality_scorer, calculate_translation_metrics_scorer])\n",
    "\n",
    "# Define multiple translation models\n",
    "models = [\n",
    "    TranslationPipeline(model=\"gpt-4o-mini\"),\n",
    "    TranslationPipeline(model=\"gpt-3.5-turbo\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models\n",
    "results = {}\n",
    "for model in models:\n",
    "    model_results = await evaluation.evaluate(model)\n",
    "    results[model.model] = model_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code creates a dataset with sample translation pairs, defines quality scorers, and runs an evaluation of our translation pipeline across multiple models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this example, we've demonstrated how to implement a translation pipeline using Large Language Models and Weave. We've shown how to:\n",
    "\n",
    "1. Create Weave operations for each step of the translation process\n",
    "2. Wrap the pipeline in a Weave Model for easy tracking and evaluation\n",
    "3. Implement custom evaluation metrics using both LLM-based and traditional approaches\n",
    "4. Create a dataset and run an evaluation of the pipeline across multiple models\n",
    "\n",
    "Weave's seamless integration allows us to track inputs, outputs, and intermediate steps throughout the translation process, making it easier to debug, optimize, and evaluate our LLM application.\n",
    "\n",
    "For more information on Weave and its capabilities, check out the [Weave documentation](https://docs.wandb.ai/weave). You can extend this example to handle larger datasets, implement more sophisticated evaluation metrics, or integrate with other LLM workflows.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
