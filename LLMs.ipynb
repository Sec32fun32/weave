{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d16c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import weave\n",
    "from weave.ecosystem.all import langchain\n",
    "weave.use_frontend_devmode()\n",
    "\n",
    "# Goals:\n",
    "#   - solve math problems\n",
    "#   - log behavior of 2 OpenAI models and Claude\n",
    "#   - try 2 different prompts each\n",
    "#   - try 2 different temperatures each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f981d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "problems = weave.save([\n",
    "    {'problem': 'integral of 2^x'},\n",
    "    {'problem': 'sqrt of pi to 10 decimals'}],\n",
    "    #{'problem': 'how many city blocks are there in boston?'}],\n",
    "    'math_problems')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eca5855",
   "metadata": {},
   "outputs": [],
   "source": [
    "problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e309654",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chat_models import ChatAnthropic\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "#llm = OpenAI(temperature=0.9)\n",
    "blank_prompt = PromptTemplate(\n",
    "    input_variables=[\"problem\"],\n",
    "    template=\"{problem}\",\n",
    ")\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"problem\"],\n",
    "    template=\"solve this problem step by step: {problem}?\",\n",
    ")\n",
    "\n",
    "#LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "chat_openai = ChatOpenAI()\n",
    "chat_anthropic = ChatAnthropic(temperature=0.7)\n",
    "models = [\n",
    "    LLMChain(llm=chat_openai, prompt=blank_prompt),\n",
    "    LLMChain(llm=chat_anthropic, prompt=blank_prompt),\n",
    "    LLMChain(llm=chat_openai, prompt=prompt)]\n",
    "prompts = ['solve this problem: ', 'solve this problem step by step: ']\n",
    "ms = weave.save(models, 'models')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f295ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a8ff18",
   "metadata": {},
   "outputs": [],
   "source": [
    "problems.map(lambda prob: ms.run(prob['problem'])).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7192ed0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19231df",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for model in models:\n",
    "    for prompt in prompts:\n",
    "        for problem in weave.use(problems):\n",
    "            results.append({\n",
    "                'model': model,\n",
    "                'temp': temp,\n",
    "                'prompt': prompt,\n",
    "                'problem': problem,\n",
    "                'result': weave.use(model.predict(prompt + problem['problem']))})\n",
    "weave.save(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd36b82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12bd7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "weave.get_results(chat_openai.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d98d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "weave.save(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a730fdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_openai = weave.save(ChatOpenAI(temperature=0.7), 'chat_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d891b670",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_anthropic = weave.save(ChatAnthropic(temperature=0.7), 'achat_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf5082b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c64b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eval_model(model, questions):\n",
    "    answers = []\n",
    "    for q in weave.use(questions):\n",
    "        a = weave.use(model.predict(q['question']))\n",
    "        answers.append({'model': model, 'question': q['question'], 'answer': a})\n",
    "    return weave.save(answers, '%s-answers' % model.type.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecf4974",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model(chat_anthropic, questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c064f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "weave.ops.get('local-artifact:///run-BaseChatModel-predict:latest/obj')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
