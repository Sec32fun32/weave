"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[577],{5379:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>s,default:()=>h,frontMatter:()=>i,metadata:()=>r,toc:()=>c});var a=n(5893),o=n(1151);const i={sidebar_position:2,hide_table_of_contents:!0},s="Tutorial: Build an Evaluation pipeline",r={id:"tutorial-eval",title:"Tutorial: Build an Evaluation pipeline",description:"To iterate on an application, we need a way to evaluate if it's improving. To do so, a common practice is to test it against the same dataset when there is a change. Weave has a first-class way to track evaluations with Dataset, Model & Evaluation classes. We have built the APIs to make minimal assumptions to allow for the flexibility to support a wide array of use-cases.",source:"@site/docs/tutorial-eval.md",sourceDirName:".",slug:"/tutorial-eval",permalink:"/weave/tutorial-eval",draft:!1,unlisted:!1,editUrl:"https://github.com/wandb/weave/blob/master/docs/docs/tutorial-eval.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2,hide_table_of_contents:!0},sidebar:"documentationSidebar",previous:{title:"Quickstart",permalink:"/weave/quickstart"},next:{title:"Weave Core Types",permalink:"/weave/guides/core-types/"}},l={},c=[{value:"Upload a <code>Dataset</code>",id:"upload-a-dataset",level:3},{value:"Build a <code>Model</code>",id:"build-a-model",level:3},{value:"Evaluate a <code>Model</code> on a <code>Dataset</code>",id:"evaluate-a-model-on-a-dataset",level:3},{value:"Pulling it all together",id:"pulling-it-all-together",level:2}];function d(e){const t={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",p:"p",pre:"pre",strong:"strong",...(0,o.a)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t.h1,{id:"tutorial-build-an-evaluation-pipeline",children:"Tutorial: Build an Evaluation pipeline"}),"\n",(0,a.jsxs)(t.p,{children:["To iterate on an application, we need a way to evaluate if it's improving. To do so, a common practice is to test it against the same dataset when there is a change. Weave has a first-class way to track evaluations with ",(0,a.jsx)(t.code,{children:"Dataset"}),", ",(0,a.jsx)(t.code,{children:"Model"})," & ",(0,a.jsx)(t.code,{children:"Evaluation"})," classes. We have built the APIs to make minimal assumptions to allow for the flexibility to support a wide array of use-cases."]}),"\n",(0,a.jsxs)(t.h3,{id:"upload-a-dataset",children:["Upload a ",(0,a.jsx)(t.code,{children:"Dataset"})]}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.code,{children:"Dataset"}),"s enable you to store examples for evaluation. Weave automatically captures when they are used and updates the ",(0,a.jsx)(t.code,{children:"Dataset"})," version when there are changes. ",(0,a.jsx)(t.code,{children:"Dataset"}),"s are created with lists of examples, where each example row is a dict."]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:"import weave\nfrom weave import weaveflow\n\nsentences = [\"There are many fruits that were found on the recently discovered planet Goocrux. There are neoskizzles that grow there, which are purple and taste like candy.\", \n\"Pounits are a bright green color and are more savory than sweet.\", \n\"Finally, there are fruits called glowls, which have a very sour and bitter taste which is acidic and caustic, and a pale orange tinge to them.\"]\nlabels = [\n    {'fruit': 'neoskizzles', 'color': 'purple', 'flavor': 'candy'},\n    {'fruit': 'pounits', 'color': 'bright green', 'flavor': 'savory'},\n    {'fruit': 'glowls', 'color': 'pale orange', 'flavor': 'sour and bitter'}\n]\n\nweave.init('intro-example')\n# highlight-next-line\ndataset = weaveflow.Dataset([\n    {'id': '0', 'sentence': sentences[0], 'extracted': labels[0]},\n    {'id': '1', 'sentence': sentences[1], 'extracted': labels[1]},\n    {'id': '2', 'sentence': sentences[2], 'extracted': labels[2]}\n])\n# highlight-next-line\ndataset_ref = weave.publish(dataset, 'example_labels')\n"})}),"\n",(0,a.jsxs)(t.p,{children:["In a new script, run this code to publish a ",(0,a.jsx)(t.code,{children:"Dataset"})," and follow the link to view it in the UI.\nIf you make edits to the ",(0,a.jsx)(t.code,{children:"Dataset"})," in the UI, you can pull the latest version in code using:"]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:"dataset = weave.ref('example_labels').get()\n"})}),"\n",(0,a.jsx)(t.admonition,{type:"note",children:(0,a.jsxs)(t.p,{children:["Checkout the ",(0,a.jsx)(t.a,{href:"/guides/core-types/datasets",children:"Datasets"})," guide to learn more."]})}),"\n",(0,a.jsxs)(t.h3,{id:"build-a-model",children:["Build a ",(0,a.jsx)(t.code,{children:"Model"})]}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.code,{children:"Model"}),"s store and version information about your system, such as prompts, temperatures, and more.\nLike ",(0,a.jsx)(t.code,{children:"Dataset"}),"s, Weave automatically captures when they are used and update the version when there are changes."]}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.code,{children:"Model"}),"s are declared by subclassing ",(0,a.jsx)(t.code,{children:"Model"})," and decorating them with ",(0,a.jsx)(t.code,{children:"@weave.type()"}),". ",(0,a.jsx)(t.code,{children:"Model"})," classes also need a ",(0,a.jsx)(t.code,{children:"predict"})," function definition, which takes one example and returns the response."]}),"\n",(0,a.jsx)(t.admonition,{type:"warning",children:(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"Known Issue"}),": If you are using Google Colab, remove ",(0,a.jsx)(t.code,{children:"async"})," from the following examples."]})}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'from weave.weaveflow import Model\nimport weave\n\n@weave.type()\n# highlight-next-line\nclass ExtractFruitsModel(Model):\n    system_message: str\n    model_name: str = "gpt-3.5-turbo-1106"\n\n    # highlight-next-line\n    @weave.op()\n    # highlight-next-line\n    async def predict(self, sentence: str) -> dict:\n        from openai import OpenAI\n        client = OpenAI()\n        response = client.chat.completions.create(\n            model=self.model_name,\n            messages=[\n                {\n                    "role": "system",\n                    "content": self.system_message\n                },\n                {\n                    "role": "user",\n                    "content": sentence\n                }\n            ],\n            temperature=0.7,\n            response_format={ "type": "json_object" }\n        )\n        extracted = response.choices[0].message.content\n        return json.loads(extracted)\n'})}),"\n",(0,a.jsxs)(t.p,{children:["You can instantiate ",(0,a.jsx)(t.code,{children:"@weave.type()"})," objects like this."]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'model = ExtractFruitsModel("You will be provided with unstructured data, and your task is to parse it one JSON dictionary with fruit, color and flavor as keys.")\nsentence = "There are many fruits that were found on the recently discovered planet Goocrux. There are neoskizzles that grow there, which are purple and taste like candy."\nprint(asyncio.run(model.predict(sentence))) \n# note: you can also call `await model.predict(sentence)` within async functions\n'})}),"\n",(0,a.jsx)(t.admonition,{type:"note",children:(0,a.jsxs)(t.p,{children:["Checkout the ",(0,a.jsx)(t.a,{href:"/guides/core-types/models",children:"Models"})," guide to learn more."]})}),"\n",(0,a.jsxs)(t.h3,{id:"evaluate-a-model-on-a-dataset",children:["Evaluate a ",(0,a.jsx)(t.code,{children:"Model"})," on a ",(0,a.jsx)(t.code,{children:"Dataset"})]}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.code,{children:"Evaluation"}),"s assess a ",(0,a.jsx)(t.code,{children:"Model"}),"s performance on a ",(0,a.jsx)(t.code,{children:"Dataset"})," using a list of specified scoring functions.\nEach scoring function takes an example row and the resulting prediction and return a dictionary of scores for that example.\n",(0,a.jsx)(t.code,{children:"example_to_model_input"})," tells ",(0,a.jsx)(t.code,{children:"evaluate"})," how to use an input from a given example row of the ",(0,a.jsx)(t.code,{children:"Dataset"}),"."]}),"\n",(0,a.jsx)(t.p,{children:"Here, we'll add two scoring functions to test the extracted data matches our labels:"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:"from weave.weaveflow import evaluate\nimport weave\n\n@weave.op()\ndef color_score(example: dict, prediction: dict) -> dict:\n    # example is a row from the Dataset, prediction is the output of predict function.\n    return {'correct': example['extracted']['color'] == prediction['color']}\n\n@weave.op()\ndef fruit_name_score(example: dict, prediction: dict) -> dict:\n    return {'correct': example['extracted']['fruit'] == prediction['fruit']}\n\n@weave.op()\ndef example_to_model_input(example: dict) -> str:\n    # example is a row from the Dataset, the output of this function should be the input to model.predict.\n    return example[\"sentence\"]\n\n# highlight-next-line\nevaluation = evaluate.Evaluation(\n    # highlight-next-line\n    dataset, scores=[color_score, fruit_name_score], example_to_model_input=example_to_model_input\n# highlight-next-line\n)\nprint(asyncio.run(evaluation.evaluate(model)))\n"})}),"\n",(0,a.jsx)(t.h2,{id:"pulling-it-all-together",children:"Pulling it all together"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:"# highlight-next-line\nimport weave\nimport asyncio\n# highlight-next-line\nfrom weave.weaveflow import Model, Evaluation, Dataset\nimport json\n\n# We create a model class with one predict function. \n# All inputs, predictions and parameters are automatically captured for easy inspection.\n@weave.type()\n# highlight-next-line\nclass ExtractFruitsModel(Model):\n    system_message: str\n    model_name: str = \"gpt-3.5-turbo-1106\"\n\n    @weave.op()\n    # highlight-next-line\n    async def predict(self, sentence: str) -> dict:\n        from openai import OpenAI\n        client = OpenAI()\n        response = client.chat.completions.create(\n            model=self.model_name,\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": self.system_message\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": sentence\n                }\n            ],\n            temperature=0.7,\n            response_format={ \"type\": \"json_object\" }\n        )\n        extracted = response.choices[0].message.content\n        return json.loads(extracted)\n\n# We call init to begin capturing data in the project, intro-example.\nweave.init('intro-example')\n\n# We create our model with our system prompt.\nmodel = ExtractFruitsModel(\"You will be provided with unstructured data, and your task is to parse it one JSON dictionary with fruit, color and flavor as keys.\")\nsentences = [\"There are many fruits that were found on the recently discovered planet Goocrux. There are neoskizzles that grow there, which are purple and taste like candy.\", \n\"Pounits are a bright green color and are more savory than sweet.\", \n\"Finally, there are fruits called glowls, which have a very sour and bitter taste which is acidic and caustic, and a pale orange tinge to them.\"]\nlabels = [\n    {'fruit': 'neoskizzles', 'color': 'purple', 'flavor': 'candy'},\n    {'fruit': 'pounits', 'color': 'bright green', 'flavor': 'savory'},\n    {'fruit': 'glowls', 'color': 'pale orange', 'flavor': 'sour and bitter'}\n]\n# Here, we track a Dataset in weave. This makes it easy to \n# automatically score a given model and compare outputs from different configurations.\n# highlight-next-line\ndataset = Dataset([\n    {'id': '0', 'sentence': sentences[0], 'extracted': labels[0]},\n    {'id': '1', 'sentence': sentences[1], 'extracted': labels[1]},\n    {'id': '2', 'sentence': sentences[2], 'extracted': labels[2]}\n])\n# highlight-next-line\ndataset_ref = weave.publish(dataset, 'example_labels')\n# If you have already published the Dataset, you can run:\n# dataset = weave.ref('example_labels').get()\n\n# We define two scoring functions to compare our model predictions with a ground truth label.\n@weave.op()\ndef color_score(example: dict, prediction: dict) -> dict:\n    # example is a row from the Dataset, prediction is the output of predict function\n    return {'correct': example['extracted']['color'] == prediction['color']}\n\n@weave.op()\ndef fruit_name_score(example: dict, prediction: dict) -> dict:\n    return {'correct': example['extracted']['fruit'] == prediction['fruit']}\n\n@weave.op()\ndef example_to_model_input(example: dict) -> str:\n    # example is a row from the Dataset, the output of this function should be the input to model.predict.\n    return example[\"sentence\"]\n\n# Finally, we run an evaluation of this model. \n# This will generate a prediction for each input example, and then score it with each scoring function.\n# highlight-next-line\nevaluation = Evaluation(\n    # highlight-next-line\n    dataset, scores=[color_score, fruit_name_score], example_to_model_input=example_to_model_input\n# highlight-next-line\n)\nprint(asyncio.run(evaluation.evaluate(model)))\n# if you're in a Jupyter Notebook, run:\n# await evaluation.evaluate(model)\n"})})]})}function h(e={}){const{wrapper:t}={...(0,o.a)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},1151:(e,t,n)=>{n.d(t,{Z:()=>r,a:()=>s});var a=n(7294);const o={},i=a.createContext(o);function s(e){const t=a.useContext(i);return a.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),a.createElement(i.Provider,{value:t},e.children)}}}]);