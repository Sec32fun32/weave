"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[4947],{89527:(e,a,n)=>{n.r(a),n.d(a,{assets:()=>d,contentTitle:()=>s,default:()=>g,frontMatter:()=>o,metadata:()=>r,toc:()=>l});var t=n(85893),i=n(11151);const o={},s="Logging media",r={id:"guides/core-types/media",title:"Logging media",description:"Weave supports logging and displaying multiple first class media types. Log images with PIL.Image.Image and audio with wave.Wave_read either directly with the object API, or as the inputs or output of an op.",source:"@site/docs/guides/core-types/media.md",sourceDirName:"guides/core-types",slug:"/guides/core-types/media",permalink:"/guides/core-types/media",draft:!1,unlisted:!1,editUrl:"https://github.com/wandb/weave/blob/master/docs/docs/guides/core-types/media.md",tags:[],version:"current",lastUpdatedAt:1730138088e3,frontMatter:{},sidebar:"documentationSidebar",previous:{title:"Costs",permalink:"/guides/tracking/costs"},next:{title:"Tools & Utilities",permalink:"/guides/tools/"}},d={},l=[{value:"Images",id:"images",level:2},{value:"Audio",id:"audio",level:2}];function c(e){const a={a:"a",code:"code",h1:"h1",h2:"h2",img:"img",p:"p",pre:"pre",...(0,i.a)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(a.h1,{id:"logging-media",children:"Logging media"}),"\n",(0,t.jsxs)(a.p,{children:["Weave supports logging and displaying multiple first class media types. Log images with ",(0,t.jsx)(a.code,{children:"PIL.Image.Image"})," and audio with ",(0,t.jsx)(a.code,{children:"wave.Wave_read"})," either directly with the object API, or as the inputs or output of an op."]}),"\n",(0,t.jsx)(a.h2,{id:"images",children:"Images"}),"\n",(0,t.jsxs)(a.p,{children:["Logging type: ",(0,t.jsx)(a.code,{children:"PIL.Image.Image"}),". Here is an example of logging an image with the OpenAI DALL-E API:"]}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-python",children:'import weave\nfrom openai import OpenAI\nimport requests\nfrom PIL import Image\n\n\nweave.init(\'image-example\')\nclient = OpenAI()\n\n@weave.op\ndef generate_image(prompt: str) -> Image:\n    response = client.images.generate(\n        model="dall-e-3",\n        prompt=prompt,\n        size="1024x1024",\n        quality="standard",\n        n=1,\n    )\n    image_url = response.data[0].url\n    image_response = requests.get(image_url, stream=True)\n    image = Image.open(image_response.raw)\n\n    # return a PIL.Image.Image object to be logged as an image\n    return image\n\ngenerate_image("a cat with a pumpkin hat")\n'})}),"\n",(0,t.jsx)(a.p,{children:"This image will be logged to weave and automatically displayed in the UI. The following is the trace view for above."}),"\n",(0,t.jsx)(a.p,{children:(0,t.jsx)(a.img,{alt:"Screenshot of pumpkin cat trace view",src:n(46026).Z+"",width:"3456",height:"1614"})}),"\n",(0,t.jsx)(a.h2,{id:"audio",children:"Audio"}),"\n",(0,t.jsxs)(a.p,{children:["Logging type: ",(0,t.jsx)(a.code,{children:"wave.Wave_read"}),". Here is an example of logging an audio file using openai's speech generation API."]}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-python",children:'import weave\nfrom openai import OpenAI\nimport wave\n\n\nweave.init("audio-example")\nclient = OpenAI()\n\n\n@weave.op\ndef make_audio_file_streaming(text: str) -> wave.Wave_read:\n    with client.audio.speech.with_streaming_response.create(\n        model="tts-1",\n        voice="alloy",\n        input=text,\n        response_format="wav",\n    ) as res:\n        res.stream_to_file("output.wav")\n\n    # return a wave.Wave_read object to be logged as audio\n    return wave.open("output.wav")\n\nmake_audio_file_streaming("Hello, how are you?")\n'})}),"\n",(0,t.jsx)(a.p,{children:"This audio will be logged to weave and automatically displayed in the UI, with an audio player. The player can be expanded to view the raw audio waveform, in addition to a download button."}),"\n",(0,t.jsx)(a.p,{children:(0,t.jsx)(a.img,{alt:"Screenshot of audio trace view",src:n(81325).Z+"",width:"3456",height:"1240"})}),"\n",(0,t.jsxs)(a.p,{children:["Try our cookbook for ",(0,t.jsx)(a.a,{href:"/reference/gen_notebooks/audio_with_weave",children:"Audio Logging"})," or ",(0,t.jsx)("a",{href:"https://colab.research.google.com/github/wandb/weave/blob/master/docs/./notebooks/audio_with_weave.ipynb",target:"_blank",rel:"noopener noreferrer",class:"navbar__item navbar__link button button--secondary button--med margin-right--sm notebook-cta-button",children:(0,t.jsxs)("div",{children:[(0,t.jsx)("img",{src:"https://upload.wikimedia.org/wikipedia/commons/archive/d/d0/20221103151430%21Google_Colaboratory_SVG_Logo.svg",alt:"Open In Colab",height:"20px"}),(0,t.jsx)("div",{children:"Open in Colab"})]})}),". The cookbook also includes an advanced example of a Real Time Audio API based assistant integrated with Weave."]})]})}function g(e={}){const{wrapper:a}={...(0,i.a)(),...e.components};return a?(0,t.jsx)(a,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},81325:(e,a,n)=>{n.d(a,{Z:()=>t});const t=n.p+"assets/images/audio-trace-ab51ad828731793ea849244477990a50.png"},46026:(e,a,n)=>{n.d(a,{Z:()=>t});const t=n.p+"assets/images/cat-pumpkin-trace-8669b2bb53dfa5c17c37e04e42cf24ef.png"},11151:(e,a,n)=>{n.d(a,{Z:()=>r,a:()=>s});var t=n(67294);const i={},o=t.createContext(i);function s(e){const a=t.useContext(o);return t.useMemo((function(){return"function"==typeof e?e(a):{...a,...e}}),[a,e])}function r(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),t.createElement(o.Provider,{value:a},e.children)}}}]);