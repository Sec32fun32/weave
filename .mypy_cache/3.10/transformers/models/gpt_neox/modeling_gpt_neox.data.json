{".class": "MypyFile", "_fullname": "transformers.models.gpt_neox.modeling_gpt_neox", "future_import_flags": [], "is_partial_stub_package": false, "is_stub": false, "names": {".class": "SymbolTable", "ACT2FN": {".class": "SymbolTableNode", "cross_ref": "transformers.activations.ACT2FN", "kind": "Gdef"}, "BaseModelOutputWithPast": {".class": "SymbolTableNode", "cross_ref": "transformers.modeling_outputs.BaseModelOutputWithPast", "kind": "Gdef"}, "CausalLMOutputWithPast": {".class": "SymbolTableNode", "cross_ref": "transformers.modeling_outputs.CausalLMOutputWithPast", "kind": "Gdef"}, "CrossEntropyLoss": {".class": "SymbolTableNode", "cross_ref": "torch.nn.modules.loss.CrossEntropyLoss", "kind": "Gdef"}, "GPTNeoXAttention": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "TypeInfo", "_promote": [], "abstract_attributes": [], "bases": ["torch.nn.modules.module.Module"], "declared_metaclass": null, "defn": {".class": "ClassDef", "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXAttention", "name": "GPTNeoXAttention", "type_vars": []}, "deletable_attributes": [], "flags": [], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXAttention", "has_param_spec_type": false, "metaclass_type": null, "metadata": {}, "module_name": "transformers.models.gpt_neox.modeling_gpt_neox", "mro": ["transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXAttention", "torch.nn.modules.module.Module", "builtins.object"], "names": {".class": "SymbolTable", "__init__": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 0], "arg_names": ["self", "config"], "flags": [], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXAttention.__init__", "name": "__init__", "type": null}}, "_attn": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 0, 0, 0, 1, 1], "arg_names": ["self", "query", "key", "value", "attention_mask", "head_mask"], "flags": [], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXAttention._attn", "name": "_attn", "type": null}}, "_merge_heads": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "Decorator", "func": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 0, 0, 0], "arg_names": ["cls", "tensor", "num_attention_heads", "attn_head_size"], "flags": ["is_class", "is_decorated"], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXAttention._merge_heads", "name": "_merge_heads", "type": null}, "is_overload": false, "var": {".class": "Var", "flags": ["is_initialized_in_class", "is_classmethod", "is_ready", "is_inferred"], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXAttention._merge_heads", "name": "_merge_heads", "type": {".class": "CallableType", "arg_kinds": [0, 0, 0, 0], "arg_names": ["cls", "tensor", "num_attention_heads", "attn_head_size"], "arg_types": [{".class": "TypeType", "item": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXAttention"}, {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 1}, {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 1}, {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 1}], "bound_args": [], "def_extras": {"first_arg": "cls"}, "fallback": "builtins.function", "from_concatenate": false, "implicit": true, "is_ellipsis_args": false, "name": "_merge_heads of GPTNeoXAttention", "ret_type": {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 1}, "type_guard": null, "unpack_kwargs": false, "variables": []}}}}, "_split_heads": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "Decorator", "func": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 0, 0, 0], "arg_names": ["cls", "tensor", "num_attention_heads", "attn_head_size"], "flags": ["is_class", "is_decorated"], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXAttention._split_heads", "name": "_split_heads", "type": null}, "is_overload": false, "var": {".class": "Var", "flags": ["is_initialized_in_class", "is_classmethod", "is_ready", "is_inferred"], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXAttention._split_heads", "name": "_split_heads", "type": {".class": "CallableType", "arg_kinds": [0, 0, 0, 0], "arg_names": ["cls", "tensor", "num_attention_heads", "attn_head_size"], "arg_types": [{".class": "TypeType", "item": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXAttention"}, {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 1}, {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 1}, {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 1}], "bound_args": [], "def_extras": {"first_arg": "cls"}, "fallback": "builtins.function", "from_concatenate": false, "implicit": true, "is_ellipsis_args": false, "name": "_split_heads of GPTNeoXAttention", "ret_type": {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 1}, "type_guard": null, "unpack_kwargs": false, "variables": []}}}}, "dense": {".class": "SymbolTableNode", "implicit": true, "kind": "Mdef", "node": {".class": "Var", "flags": ["is_inferred"], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXAttention.dense", "name": "dense", "type": {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 1}}}, "forward": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 0, 0, 1, 1, 1, 1], "arg_names": ["self", "hidden_states", "attention_mask", "head_mask", "layer_past", "use_cache", "output_attentions"], "flags": [], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXAttention.forward", "name": "forward", "type": null}}, "head_size": {".class": "SymbolTableNode", "implicit": true, "kind": "Mdef", "node": {".class": "Var", "flags": ["is_inferred"], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXAttention.head_size", "name": "head_size", "type": {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 1}}}, "hidden_size": {".class": "SymbolTableNode", "implicit": true, "kind": "Mdef", "node": {".class": "Var", "flags": ["is_inferred"], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXAttention.hidden_size", "name": "hidden_size", "type": {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 1}}}, "norm_factor": {".class": "SymbolTableNode", "implicit": true, "kind": "Mdef", "node": {".class": "Var", "flags": ["is_inferred"], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXAttention.norm_factor", "name": "norm_factor", "type": {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 1}}}, "num_attention_heads": {".class": "SymbolTableNode", "implicit": true, "kind": "Mdef", "node": {".class": "Var", "flags": ["is_inferred"], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXAttention.num_attention_heads", "name": "num_attention_heads", "type": {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 1}}}, "query_key_value": {".class": "SymbolTableNode", "implicit": true, "kind": "Mdef", "node": {".class": "Var", "flags": ["is_inferred"], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXAttention.query_key_value", "name": "query_key_value", "type": {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 1}}}, "rotary_emb": {".class": "SymbolTableNode", "implicit": true, "kind": "Mdef", "node": {".class": "Var", "flags": ["is_inferred"], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXAttention.rotary_emb", "name": "rotary_emb", "type": {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 1}}}, "rotary_ndims": {".class": "SymbolTableNode", "implicit": true, "kind": "Mdef", "node": {".class": "Var", "flags": ["is_inferred"], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXAttention.rotary_ndims", "name": "rotary_ndims", "type": {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 1}}}}, "slots": null, "tuple_type": null, "type_vars": [], "typeddict_type": null}}, "GPTNeoXConfig": {".class": "SymbolTableNode", "cross_ref": "transformers.models.gpt_neox.configuration_gpt_neox.GPTNeoXConfig", "kind": "Gdef"}, "GPTNeoXForCausalLM": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "TypeInfo", "_promote": [], "abstract_attributes": [], "bases": ["transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXPreTrainedModel"], "declared_metaclass": null, "defn": {".class": "ClassDef", "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXForCausalLM", "name": "GPTNeoXForCausalLM", "type_vars": []}, "deletable_attributes": [], "flags": [], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXForCausalLM", "has_param_spec_type": false, "metaclass_type": null, "metadata": {}, "module_name": "transformers.models.gpt_neox.modeling_gpt_neox", "mro": ["transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXForCausalLM", "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXPreTrainedModel", "transformers.modeling_utils.PreTrainedModel", "torch.nn.modules.module.Module", "transformers.modeling_utils.ModuleUtilsMixin", "transformers.generation_utils.GenerationMixin", "transformers.utils.hub.PushToHubMixin", "builtins.object"], "names": {".class": "SymbolTable", "__init__": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 0], "arg_names": ["self", "config"], "flags": [], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXForCausalLM.__init__", "name": "__init__", "type": null}}, "_keys_to_ignore_on_load_missing": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "Var", "flags": ["is_initialized_in_class", "is_inferred", "has_explicit_value"], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXForCausalLM._keys_to_ignore_on_load_missing", "name": "_keys_to_ignore_on_load_missing", "type": null}}, "_reorder_cache": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 0, 0], "arg_names": ["self", "past", "beam_idx"], "flags": [], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXForCausalLM._reorder_cache", "name": "_reorder_cache", "type": null}}, "embed_out": {".class": "SymbolTableNode", "implicit": true, "kind": "Mdef", "node": {".class": "Var", "flags": ["is_inferred"], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXForCausalLM.embed_out", "name": "embed_out", "type": {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 1}}}, "forward": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "Decorator", "func": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "arg_names": ["self", "input_ids", "attention_mask", "inputs_embeds", "head_mask", "past_key_values", "labels", "use_cache", "output_attentions", "output_hidden_states", "return_dict"], "flags": ["is_decorated"], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXForCausalLM.forward", "name": "forward", "type": null}, "is_overload": false, "var": {".class": "Var", "flags": ["is_initialized_in_class", "is_ready", "is_inferred"], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXForCausalLM.forward", "name": "forward", "type": {".class": "AnyType", "missing_import_name": null, "source_any": {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 1}, "type_of_any": 7}}}}, "get_output_embeddings": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0], "arg_names": ["self"], "flags": [], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXForCausalLM.get_output_embeddings", "name": "get_output_embeddings", "type": null}}, "gpt_neox": {".class": "SymbolTableNode", "implicit": true, "kind": "Mdef", "node": {".class": "Var", "flags": ["is_inferred"], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXForCausalLM.gpt_neox", "name": "gpt_neox", "type": {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 1}}}, "prepare_inputs_for_generation": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 0, 1, 1, 4], "arg_names": ["self", "input_ids", "past", "attention_mask", "model_kwargs"], "flags": [], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXForCausalLM.prepare_inputs_for_generation", "name": "prepare_inputs_for_generation", "type": null}}, "set_output_embeddings": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 0], "arg_names": ["self", "new_embeddings"], "flags": [], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXForCausalLM.set_output_embeddings", "name": "set_output_embeddings", "type": null}}}, "slots": null, "tuple_type": null, "type_vars": [], "typeddict_type": null}}, "GPTNeoXLayer": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "TypeInfo", "_promote": [], "abstract_attributes": [], "bases": ["torch.nn.modules.module.Module"], "declared_metaclass": null, "defn": {".class": "ClassDef", "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXLayer", "name": "GPTNeoXLayer", "type_vars": []}, "deletable_attributes": [], "flags": [], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXLayer", "has_param_spec_type": false, "metaclass_type": null, "metadata": {}, "module_name": "transformers.models.gpt_neox.modeling_gpt_neox", "mro": ["transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXLayer", "torch.nn.modules.module.Module", "builtins.object"], "names": {".class": "SymbolTable", "__init__": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 0], "arg_names": ["self", "config"], "flags": [], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXLayer.__init__", "name": "__init__", "type": null}}, "attention": {".class": "SymbolTableNode", "implicit": true, "kind": "Mdef", "node": {".class": "Var", "flags": ["is_inferred"], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXLayer.attention", "name": "attention", "type": {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 1}}}, "forward": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 0, 1, 1, 1, 1, 1], "arg_names": ["self", "hidden_states", "attention_mask", "head_mask", "use_cache", "layer_past", "output_attentions"], "flags": [], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXLayer.forward", "name": "forward", "type": null}}, "input_layernorm": {".class": "SymbolTableNode", "implicit": true, "kind": "Mdef", "node": {".class": "Var", "flags": ["is_inferred"], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXLayer.input_layernorm", "name": "input_layernorm", "type": {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 1}}}, "mlp": {".class": "SymbolTableNode", "implicit": true, "kind": "Mdef", "node": {".class": "Var", "flags": ["is_inferred"], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXLayer.mlp", "name": "mlp", "type": {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 1}}}, "post_attention_layernorm": {".class": "SymbolTableNode", "implicit": true, "kind": "Mdef", "node": {".class": "Var", "flags": ["is_inferred"], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXLayer.post_attention_layernorm", "name": "post_attention_layernorm", "type": {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 1}}}}, "slots": null, "tuple_type": null, "type_vars": [], "typeddict_type": null}}, "GPTNeoXMLP": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "TypeInfo", "_promote": [], "abstract_attributes": [], "bases": ["torch.nn.modules.module.Module"], "declared_metaclass": null, "defn": {".class": "ClassDef", "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXMLP", "name": "GPTNeoXMLP", "type_vars": []}, "deletable_attributes": [], "flags": [], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXMLP", "has_param_spec_type": false, "metaclass_type": null, "metadata": {}, "module_name": "transformers.models.gpt_neox.modeling_gpt_neox", "mro": ["transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXMLP", "torch.nn.modules.module.Module", "builtins.object"], "names": {".class": "SymbolTable", "__init__": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 0], "arg_names": ["self", "config"], "flags": [], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXMLP.__init__", "name": "__init__", "type": null}}, "act": {".class": "SymbolTableNode", "implicit": true, "kind": "Mdef", "node": {".class": "Var", "flags": ["is_inferred"], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXMLP.act", "name": "act", "type": {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 1}}}, "dense_4h_to_h": {".class": "SymbolTableNode", "implicit": true, "kind": "Mdef", "node": {".class": "Var", "flags": ["is_inferred"], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXMLP.dense_4h_to_h", "name": "dense_4h_to_h", "type": {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 1}}}, "dense_h_to_4h": {".class": "SymbolTableNode", "implicit": true, "kind": "Mdef", "node": {".class": "Var", "flags": ["is_inferred"], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXMLP.dense_h_to_4h", "name": "dense_h_to_4h", "type": {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 1}}}, "forward": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 0], "arg_names": ["self", "hidden_states"], "flags": [], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXMLP.forward", "name": "forward", "type": null}}}, "slots": null, "tuple_type": null, "type_vars": [], "typeddict_type": null}}, "GPTNeoXModel": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "TypeInfo", "_promote": [], "abstract_attributes": [], "bases": ["transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXPreTrainedModel"], "declared_metaclass": null, "defn": {".class": "ClassDef", "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXModel", "name": "GPTNeoXModel", "type_vars": []}, "deletable_attributes": [], "flags": [], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXModel", "has_param_spec_type": false, "metaclass_type": null, "metadata": {}, "module_name": "transformers.models.gpt_neox.modeling_gpt_neox", "mro": ["transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXModel", "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXPreTrainedModel", "transformers.modeling_utils.PreTrainedModel", "torch.nn.modules.module.Module", "transformers.modeling_utils.ModuleUtilsMixin", "transformers.generation_utils.GenerationMixin", "transformers.utils.hub.PushToHubMixin", "builtins.object"], "names": {".class": "SymbolTable", "__init__": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 0], "arg_names": ["self", "config"], "flags": [], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXModel.__init__", "name": "__init__", "type": null}}, "embed_in": {".class": "SymbolTableNode", "implicit": true, "kind": "Mdef", "node": {".class": "Var", "flags": ["is_inferred"], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXModel.embed_in", "name": "embed_in", "type": {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 1}}}, "final_layer_norm": {".class": "SymbolTableNode", "implicit": true, "kind": "Mdef", "node": {".class": "Var", "flags": ["is_inferred"], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXModel.final_layer_norm", "name": "final_layer_norm", "type": {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 1}}}, "forward": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "Decorator", "func": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 1, 1, 1, 1, 1, 1, 1, 1, 1], "arg_names": ["self", "input_ids", "attention_mask", "head_mask", "inputs_embeds", "past_key_values", "use_cache", "output_attentions", "output_hidden_states", "return_dict"], "flags": ["is_decorated"], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXModel.forward", "name": "forward", "type": null}, "is_overload": false, "var": {".class": "Var", "flags": ["is_initialized_in_class", "is_ready", "is_inferred"], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXModel.forward", "name": "forward", "type": {".class": "AnyType", "missing_import_name": null, "source_any": {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 1}, "type_of_any": 7}}}}, "get_input_embeddings": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0], "arg_names": ["self"], "flags": [], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXModel.get_input_embeddings", "name": "get_input_embeddings", "type": null}}, "layers": {".class": "SymbolTableNode", "implicit": true, "kind": "Mdef", "node": {".class": "Var", "flags": ["is_inferred"], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXModel.layers", "name": "layers", "type": {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 1}}}, "set_input_embeddings": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 0], "arg_names": ["self", "value"], "flags": [], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXModel.set_input_embeddings", "name": "set_input_embeddings", "type": null}}}, "slots": null, "tuple_type": null, "type_vars": [], "typeddict_type": null}}, "GPTNeoXPreTrainedModel": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "TypeInfo", "_promote": [], "abstract_attributes": [], "bases": ["transformers.modeling_utils.PreTrainedModel"], "declared_metaclass": null, "defn": {".class": "ClassDef", "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXPreTrainedModel", "name": "GPTNeoXPreTrainedModel", "type_vars": []}, "deletable_attributes": [], "flags": [], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXPreTrainedModel", "has_param_spec_type": false, "metaclass_type": null, "metadata": {}, "module_name": "transformers.models.gpt_neox.modeling_gpt_neox", "mro": ["transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXPreTrainedModel", "transformers.modeling_utils.PreTrainedModel", "torch.nn.modules.module.Module", "transformers.modeling_utils.ModuleUtilsMixin", "transformers.generation_utils.GenerationMixin", "transformers.utils.hub.PushToHubMixin", "builtins.object"], "names": {".class": "SymbolTable", "_init_weights": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 0], "arg_names": ["self", "module"], "flags": [], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXPreTrainedModel._init_weights", "name": "_init_weights", "type": null}}, "_no_split_modules": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "Var", "flags": ["is_initialized_in_class", "is_inferred", "has_explicit_value"], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXPreTrainedModel._no_split_modules", "name": "_no_split_modules", "type": null}}, "_set_gradient_checkpointing": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 0, 1], "arg_names": ["self", "module", "value"], "flags": [], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXPreTrainedModel._set_gradient_checkpointing", "name": "_set_gradient_checkpointing", "type": null}}, "base_model_prefix": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "Var", "flags": ["is_initialized_in_class", "is_ready", "is_inferred", "has_explicit_value"], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXPreTrainedModel.base_model_prefix", "name": "base_model_prefix", "type": "builtins.str"}}, "config_class": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "Var", "flags": ["is_initialized_in_class", "is_inferred", "has_explicit_value"], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXPreTrainedModel.config_class", "name": "config_class", "type": null}}, "supports_gradient_checkpointing": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "Var", "flags": ["is_initialized_in_class", "is_inferred", "has_explicit_value"], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXPreTrainedModel.supports_gradient_checkpointing", "name": "supports_gradient_checkpointing", "type": "builtins.bool"}}}, "slots": null, "tuple_type": null, "type_vars": [], "typeddict_type": null}}, "GPT_NEOX_INPUTS_DOCSTRING": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "Var", "flags": ["is_ready", "is_inferred", "has_explicit_value"], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPT_NEOX_INPUTS_DOCSTRING", "name": "GPT_NEOX_INPUTS_DOCSTRING", "type": "builtins.str"}}, "GPT_NEOX_PRETRAINED_MODEL_ARCHIVE_LIST": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "Var", "flags": ["is_inferred", "has_explicit_value"], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPT_NEOX_PRETRAINED_MODEL_ARCHIVE_LIST", "name": "GPT_NEOX_PRETRAINED_MODEL_ARCHIVE_LIST", "type": {".class": "Instance", "args": ["builtins.str"], "type_ref": "builtins.list"}}}, "GPT_NEOX_START_DOCSTRING": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "Var", "flags": ["is_ready", "is_inferred", "has_explicit_value"], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.GPT_NEOX_START_DOCSTRING", "name": "GPT_NEOX_START_DOCSTRING", "type": "builtins.str"}}, "PreTrainedModel": {".class": "SymbolTableNode", "cross_ref": "transformers.modeling_utils.PreTrainedModel", "kind": "Gdef"}, "RotaryEmbedding": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "TypeInfo", "_promote": [], "abstract_attributes": [], "bases": ["torch.nn.modules.module.Module"], "declared_metaclass": null, "defn": {".class": "ClassDef", "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.RotaryEmbedding", "name": "RotaryEmbedding", "type_vars": []}, "deletable_attributes": [], "flags": [], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.RotaryEmbedding", "has_param_spec_type": false, "metaclass_type": null, "metadata": {}, "module_name": "transformers.models.gpt_neox.modeling_gpt_neox", "mro": ["transformers.models.gpt_neox.modeling_gpt_neox.RotaryEmbedding", "torch.nn.modules.module.Module", "builtins.object"], "names": {".class": "SymbolTable", "__init__": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 0, 1, 1], "arg_names": ["self", "dim", "base", "device"], "flags": [], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.RotaryEmbedding.__init__", "name": "__init__", "type": null}}, "cos_cached": {".class": "SymbolTableNode", "implicit": true, "kind": "Mdef", "node": {".class": "Var", "flags": ["is_inferred"], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.RotaryEmbedding.cos_cached", "name": "cos_cached", "type": {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 1}}}, "forward": {".class": "SymbolTableNode", "kind": "Mdef", "node": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 0, 1], "arg_names": ["self", "x", "seq_len"], "flags": [], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.RotaryEmbedding.forward", "name": "forward", "type": null}}, "max_seq_len_cached": {".class": "SymbolTableNode", "implicit": true, "kind": "Mdef", "node": {".class": "Var", "flags": ["is_inferred"], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.RotaryEmbedding.max_seq_len_cached", "name": "max_seq_len_cached", "type": {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 1}}}, "sin_cached": {".class": "SymbolTableNode", "implicit": true, "kind": "Mdef", "node": {".class": "Var", "flags": ["is_inferred"], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.RotaryEmbedding.sin_cached", "name": "sin_cached", "type": {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 1}}}}, "slots": null, "tuple_type": null, "type_vars": [], "typeddict_type": null}}, "_CHECKPOINT_FOR_DOC": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "Var", "flags": ["is_ready", "is_inferred", "has_explicit_value"], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox._CHECKPOINT_FOR_DOC", "name": "_CHECKPOINT_FOR_DOC", "type": "builtins.str"}}, "_CONFIG_FOR_DOC": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "Var", "flags": ["is_ready", "is_inferred", "has_explicit_value"], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox._CONFIG_FOR_DOC", "name": "_CONFIG_FOR_DOC", "type": "builtins.str"}}, "_TOKENIZER_FOR_DOC": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "Var", "flags": ["is_ready", "is_inferred", "has_explicit_value"], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox._TOKENIZER_FOR_DOC", "name": "_TOKENIZER_FOR_DOC", "type": "builtins.str"}}, "__annotations__": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "Var", "flags": ["is_ready"], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.__annotations__", "name": "__annotations__", "type": {".class": "Instance", "args": ["builtins.str", {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 6}], "type_ref": "builtins.dict"}}}, "__doc__": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "Var", "flags": ["is_ready"], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.__doc__", "name": "__doc__", "type": "builtins.str"}}, "__file__": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "Var", "flags": ["is_ready"], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.__file__", "name": "__file__", "type": "builtins.str"}}, "__name__": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "Var", "flags": ["is_ready"], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.__name__", "name": "__name__", "type": "builtins.str"}}, "__package__": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "Var", "flags": ["is_ready"], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.__package__", "name": "__package__", "type": "builtins.str"}}, "add_code_sample_docstrings": {".class": "SymbolTableNode", "cross_ref": "transformers.utils.doc.add_code_sample_docstrings", "kind": "Gdef"}, "add_start_docstrings": {".class": "SymbolTableNode", "cross_ref": "transformers.utils.doc.add_start_docstrings", "kind": "Gdef"}, "add_start_docstrings_to_model_forward": {".class": "SymbolTableNode", "cross_ref": "transformers.utils.doc.add_start_docstrings_to_model_forward", "kind": "Gdef"}, "apply_rotary_pos_emb": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 0, 0, 0, 1], "arg_names": ["q", "k", "cos", "sin", "offset"], "flags": [], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.apply_rotary_pos_emb", "name": "apply_rotary_pos_emb", "type": {".class": "CallableType", "arg_kinds": [0, 0, 0, 0, 1], "arg_names": ["q", "k", "cos", "sin", "offset"], "arg_types": [{".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 1}, {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 1}, {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 1}, {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 1}, "builtins.int"], "bound_args": [], "def_extras": {"first_arg": null}, "fallback": "builtins.function", "from_concatenate": false, "implicit": false, "is_ellipsis_args": false, "name": "apply_rotary_pos_emb", "ret_type": {".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 1}, "type_guard": null, "unpack_kwargs": false, "variables": []}}}, "attention_mask_func": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0, 0], "arg_names": ["attention_scores", "ltor_mask"], "flags": [], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.attention_mask_func", "name": "attention_mask_func", "type": null}}, "logger": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "Var", "flags": ["is_inferred", "has_explicit_value"], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.logger", "name": "logger", "type": "logging.Logger"}}, "logging": {".class": "SymbolTableNode", "cross_ref": "transformers.utils.logging", "kind": "Gdef"}, "nn": {".class": "SymbolTableNode", "cross_ref": "torch.nn", "kind": "Gdef"}, "replace_return_docstrings": {".class": "SymbolTableNode", "cross_ref": "transformers.utils.doc.replace_return_docstrings", "kind": "Gdef"}, "rotate_half": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "FuncDef", "abstract_status": 0, "arg_kinds": [0], "arg_names": ["x"], "flags": [], "fullname": "transformers.models.gpt_neox.modeling_gpt_neox.rotate_half", "name": "rotate_half", "type": null}}, "torch": {".class": "SymbolTableNode", "cross_ref": "torch", "kind": "Gdef"}}, "path": "/home/paperspace/.pyenv/versions/3.10.4/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py"}